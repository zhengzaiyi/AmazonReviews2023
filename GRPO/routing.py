import json
import random
from dataclasses import dataclass
from typing import Dict, List, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from GRPO.agents import LLMRouterAgent

from .utils import recall_at_k, merge_candidates, ndcg_at_k
from collections import defaultdict


def generate_diverse_routes(router, prof_json, n_candidates, temperature=0.8, include_logprobs=False):

    if not include_logprobs:
        # Original behavior for backward compatibility
        return router.forward(
            prof_json, 
            n_candidates=n_candidates, 
            temperature=temperature, 
            ensure_diversity=True
        )
    
    # Enhanced behavior with logprob collection
    # Get logprobs directly from the generate calls to avoid redundant computation
    route_logprobs = []
    ref_route_logprobs = []
    
    # GRPO: Only ref_model generates samples (this is the sampling phase)
    ref_result = router.local_hf.generate(
        prof_json,
        n_candidates, 
        router.available_models,
        temperature=temperature,
        num_return_sequences=n_candidates,
        ref_mode=True
    )
    
    routes = ref_result["routes"]
    ref_logprobs = ref_result["logprobs"]  # Logprobs from ref_model generation
    
    # GRPO: Current model evaluates the fixed samples generated by ref_model
    for i, route in enumerate(routes):
        # Ref model logprob (use cached value directly)
        ref_logprob = ref_logprobs[i] if i < len(ref_logprobs) else 0.0
        
        # Policy model logprob (computed fresh for the same route)
        policy_result_dict = router.local_hf.get_logprob(prof_json, json.dumps(route))
        policy_logprob = policy_result_dict['total_logprob']
        
        route_logprobs.append(policy_logprob)
        ref_route_logprobs.append(ref_logprob)
    
    # Apply diversity selection
    if len(routes) > n_candidates:
        diverse_routes = router._select_diverse_routes(routes, n_candidates)
        route_indices = [routes.index(r) for r in diverse_routes]
        route_logprobs = [route_logprobs[i] for i in route_indices]
        ref_route_logprobs = [ref_route_logprobs[i] for i in route_indices]
        routes = diverse_routes
    
    return {
        "routes": routes,
        "route_logprobs": route_logprobs,
        "ref_route_logprobs": ref_route_logprobs
    }




@dataclass
class GRPOConfig:
    beta: float = 1.0
    group_size: int = 4
    lr: float = 3e-4
    polyak_tau: float = 0.01


class GRPOTrainer:
    def __init__(self, cfg: GRPOConfig, router=None):
        self.cfg = cfg
        
        # Router training support
        self.router = router
        self.router_opt = None
        if router is not None and hasattr(router, 'local_hf'):
            self.router_opt = torch.optim.Adam(router.local_hf.model.parameters(), lr=cfg.lr)

    def step_router_only(self,
                        batch_users: List[int],
                        histories: Dict[int, List[int]],
                        user2items_test: Dict[int, List[int]],
                        profile_agent,
                        recallers: Dict[str, object],
                        final_k: int = 50) -> Dict[str, float]:
        """
        GRPO training for router-only mode, where we train the router (LLM) directly
        """
        if self.router is None or self.router_opt is None:
            raise ValueError("Router is not set up for training. Please provide a trainable router in __init__.")
        
        losses, rewards_all = [], []
        for uid in batch_users:
            hist = histories.get(uid, [])
            prof_json = profile_agent.forward(uid, hist).profile_json
            
            # Get routes from the trainable router using improved diversity generation with logprobs
            route_data = generate_diverse_routes(self.router, prof_json, n_candidates=self.cfg.group_size, include_logprobs=True)
            routes = route_data["routes"]
            route_logprobs = route_data["route_logprobs"] 
            ref_route_logprobs = route_data["ref_route_logprobs"]
            
            if len(routes) < 2:
                continue
                
            # Evaluate each route
            rewards = []
            for r in routes:
                candidates = defaultdict(int)
                for model_usage in r:
                    model_name = model_usage["name"]
                    k = model_usage["k"]
                    w = model_usage["weight"]
                    if model_name not in recallers:
                        continue
                    recall_list = recallers.get(model_name, None).recall(uid, int(k), hist)
                    for item in recall_list:
                        item[1] *= w
                        candidates[item[0]] += item[1]

                candidates = sorted(candidates.keys(), key=lambda x: candidates[x], reverse=True)
                candidates = candidates[:final_k]
                rr = ndcg_at_k(candidates, user2items_test.get(uid, []), k=final_k)
                rewards.append(rr)
            
            rewards_all.extend(rewards)
            if not route_logprobs:  # Fallback if router doesn't support logprob extraction
                continue
                
            # GRPO: Calculate group-relative advantages
            baseline = np.mean(rewards)  # Group baseline (average reward)
            advantages = [reward - baseline for reward in rewards]  # Relative advantages
            
            # GRPO loss calculation: weighted policy gradient
            for j, advantage in enumerate(advantages):
                if abs(advantage) < 1e-6:  # Skip neutral samples
                    continue
                
                # Re-compute policy logprob with gradients for this specific route
                route_json = json.dumps(routes[j])
                full_text = prof_json + route_json
                
                inputs = self.router.local_hf.tokenizer(full_text, return_tensors="pt").to(self.router.local_hf.model.device)
                labels = inputs["input_ids"].clone()
                labels[labels == self.router.local_hf.tokenizer.pad_token_id] = -100
                
                outputs = self.router.local_hf.model(**inputs, labels=labels, return_dict=True)
                loss_ce = outputs.loss
                policy_logprob = -loss_ce * (labels != -100).sum().item()  # Convert CE loss to total logprob
                
                ref_logprob = ref_route_logprobs[j]
                
                # KL divergence regularization: policy vs reference
                kl_div = policy_logprob - ref_logprob
                
                # GRPO objective: maximize advantage-weighted log probability with KL penalty  
                objective = advantage * policy_logprob - self.cfg.beta * kl_div
                loss = -objective  # Gradient ascent -> descent
                losses.append(loss)
        
        if not losses:
            return {"loss": 0.0, "avg_recall": float(np.mean(rewards_all) if rewards_all else 0.0)}
        
        # Backpropagation for router
        loss = torch.stack(losses).mean()
        self.router_opt.zero_grad()
        loss.backward()
        
        # Gradient clipping for router
        if hasattr(self.router, 'parameters'):
            nn.utils.clip_grad_norm_(self.router.parameters(), 1.0)
        
        self.router_opt.step()
        
        # Update reference router with polyak averaging
        with torch.no_grad():
            tau = self.cfg.polyak_tau
            for p, q in zip(self.router.local_hf.ref_model.parameters(), self.router.local_hf.model.parameters()):
                p.data.mul_(1 - tau).add_(tau * q.data)
        
        return {"loss": float(loss.item()), "avg_recall": float(np.mean(rewards_all) if rewards_all else 0.0)}

    def train_router(self, 
                    users: List[int],
                    histories: Dict[int, List[int]],
                    user2items_test: Dict[int, List[int]],
                    profile_agent,
                    recallers: Dict[str, object],
                    final_k: int = 50,
                    batch_size: int = 32) -> Dict[str, float]:
        """Train router using GRPO in batches"""
        if self.router is None or self.router_opt is None:
            raise ValueError("Router is not set up for training.")
        
        total_loss = 0.0
        num_batches = 0
        logs = []
        
        for i in range(0, len(users), batch_size):
            batch_users = users[i:i+batch_size]
            result = self.step_router_only(
                batch_users=batch_users,
                histories=histories,
                user2items_test=user2items_test,
                profile_agent=profile_agent,
                recallers=recallers,
                final_k=final_k
            )
            total_loss += result["loss"]
            logs.extend([result["avg_recall"]] * len(batch_users))
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        avg_recall = float(np.mean(logs)) if logs else 0.0
        
        return {"avg_recall": avg_recall, "avg_loss": avg_loss}

    def evaluate_router(self,
                       users: List[int],
                       histories: Dict[int, List[int]],
                       user2items_test: Dict[int, List[int]],
                       profile_agent,
                       router,
                       recallers: Dict[str, object],
                       final_k: int = 50,
                       group_size: int = 4,
                       strategy: str = "first",
                       save_router_json: Optional[str] = None) -> Dict[str, float]:
        """Evaluate router performance without training"""
        logs = []
        routes_dump = {}
        
        for uid in users:
            hist = histories.get(uid, [])
            prof_json = profile_agent.forward(uid, hist).profile_json
            routes = generate_diverse_routes(router, prof_json, n_candidates=group_size)
            
            if save_router_json is not None:
                routes_dump[str(uid)] = routes
            if not routes:
                continue
                
            # Select route based on strategy
            chosen = routes[0]
            if strategy == "oracle" and len(routes) > 1:
                scores = []
                for route in routes:
                    candidates = defaultdict(float)
                    for model_usage in route:
                        model_name = model_usage["name"]
                        k = model_usage["k"]
                        w = model_usage["weight"]
                        if model_name in recallers:
                            recall_list = recallers[model_name].recall(uid, int(k), hist)
                            for item in recall_list:
                                candidates[item[0]] += item[1] * w
                    candidates = sorted(candidates.keys(), key=lambda x: candidates[x], reverse=True)[:final_k]
                    scores.append(recall_at_k(candidates, user2items_test.get(uid, []), k=final_k))
                chosen = routes[int(np.argmax(scores))] if scores else routes[0]
            
            # Evaluate chosen route
            candidates = defaultdict(float)
            for model_usage in chosen:
                model_name = model_usage["name"]
                k = model_usage["k"]
                w = model_usage["weight"]
                if model_name in recallers:
                    recall_list = recallers[model_name].recall(uid, int(k), hist)
                    for item in recall_list:
                        candidates[item[0]] += item[1] * w
            candidates = sorted(candidates.keys(), key=lambda x: candidates[x], reverse=True)[:final_k]
            logs.append(recall_at_k(candidates, user2items_test.get(uid, []), k=final_k))
            
        # Save routes if requested
        if save_router_json is not None:
            with open(save_router_json, "w", encoding="utf-8") as f:
                json.dump(routes_dump, f, ensure_ascii=False)
        
        return {"avg_recall": float(np.mean(logs)) if logs else 0.0}

    def run(self, 
           users: List[int],
           histories: Dict[int, List[int]],
           user2items_test: Dict[int, List[int]],
           profile_agent,
           router,
           recallers: Dict[str, object],
           final_k: int = 50,
           group_size: int = 4,
           strategy: str = "first",
           save_router_json: Optional[str] = None,
           train_mode: bool = False,
           batch_size: int = 32) -> Dict[str, float]:
        """Unified interface for router training and evaluation"""
        if train_mode:
            return self.train_router(users, histories, user2items_test, profile_agent, 
                                   recallers, final_k, batch_size)
        else:
            return self.evaluate_router(users, histories, user2items_test, profile_agent,
                                      router, recallers, final_k, group_size, strategy, save_router_json)

