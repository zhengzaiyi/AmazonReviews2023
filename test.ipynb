{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5babfa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.general_recommender import BPR, Pop, ItemKNN, LightGCN\n",
    "from recbole.model.sequential_recommender import SASRec\n",
    "from recbole.utils import init_seed, init_logger, get_trainer\n",
    "\n",
    "# Set torch.load compatibility\n",
    "torch.serialization.add_safe_globals([dict, list, tuple, set])\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26543fb4",
   "metadata": {},
   "source": [
    "## Recaller Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon Reviews 2023 dataset using RecBole\n",
    "from GRPO.data import get_base_config_dict\n",
    "import os\n",
    "for dataset_name in os.listdir('dataset'):\n",
    "    try:\n",
    "        config_5core = Config(\n",
    "            model='SASRec',\n",
    "            dataset=dataset_name, \n",
    "            config_dict=get_base_config_dict(dataset_name)\n",
    "        )\n",
    "\n",
    "        # ÂàõÂª∫5-coreËøáÊª§ÂêéÁöÑÊï∞ÊçÆÈõÜ\n",
    "        print(f\"=== Loading {dataset_name} ===\")\n",
    "        print(\"Creating 5-core filtered dataset...\")\n",
    "        from recbole.utils import init_seed as recbole_init_seed\n",
    "        recbole_init_seed(seed=42, reproducibility=True)\n",
    "        dataset_5core = create_dataset(config_5core)\n",
    "        train_data_5core, valid_data_5core, test_data_5core = data_preparation(config_5core, dataset_5core)\n",
    "        print(f\"\\nüìä 5-core Filtered Dataset Statistics:\")\n",
    "        print(dataset_5core)\n",
    "        # train set stats\n",
    "        import numpy as np\n",
    "        print(train_data_5core.dataset)\n",
    "        print(np.unique(train_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "        print(valid_data_5core.dataset)\n",
    "        print(np.unique(valid_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "        print(test_data_5core.dataset)\n",
    "        print(np.unique(test_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22db084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified training function defined!\n"
     ]
    }
   ],
   "source": [
    "# Define unified model training function\n",
    "def train_model(model_type, dataset_name='All_Beauty', epochs=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Unified function to train recommendation models\n",
    "    \n",
    "    Args:\n",
    "        model_type: Model type ('BPR', 'SASRec', 'Pop')\n",
    "        dataset_name: Dataset name\n",
    "        epochs: Training epochs¬∑\n",
    "        **kwargs: Additional model-specific parameters\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, trainer, config and results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Training {model_type} Model ===\")\n",
    "    \n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        # 'data_path': 'seq_rec_results/dataset/processed/',\n",
    "        # 'benchmark_filename': ['train', 'valid', 'test'],\n",
    "        'epochs': epochs,\n",
    "        'stopping_step': 10,\n",
    "        'eval_step': 1,\n",
    "        'metrics': ['Recall', 'NDCG'],\n",
    "        'topk': [10, 50],\n",
    "        'valid_metric': 'NDCG@10',\n",
    "        'checkpoint_dir': './checkpoints/',\n",
    "        'show_progress': True,\n",
    "        'save_dataset': True,\n",
    "        'save_dataloaders': True,\n",
    "    }\n",
    "    base_config.update({\n",
    "        'data_path': 'dataset',\n",
    "        'load_col': {\n",
    "            # 'inter': ['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "            'inter': ['user_id', 'item_id', 'timestamp']\n",
    "        },\n",
    "        'user_inter_num_interval': \"[5,inf)\",\n",
    "        'item_inter_num_interval': \"[5,inf)\",\n",
    "        'train_neg_sample_args': None,\n",
    "        'loss_type': 'CE',\n",
    "        # 'val_interval': {\n",
    "        #     'rating': '[3,inf)'  # Âè™‰øùÁïôrating >= 4ÁöÑ‰∫§‰∫í\n",
    "        # },\n",
    "        'eval_args': {\n",
    "            'split': {'LS': 'valid_and_test'},  # Leave-One-Out\n",
    "            'order': 'TO',  # Temporal Order\n",
    "            'group_by': 'user'\n",
    "        },\n",
    "        'ITEM_ID_FIELD': 'item_id',\n",
    "    })\n",
    "    \n",
    "    # Model-specific configurations\n",
    "    if model_type == 'BPR':\n",
    "        model_class = BPR\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,\n",
    "                'alpha': 1.0,\n",
    "                'dynamic': False,\n",
    "                'candidate_num': 0\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "        \n",
    "    elif model_type == 'SASRec':\n",
    "        model_class = SASRec\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "            'loss_type': 'CE',\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 256,\n",
    "            'max_seq_length': 50,\n",
    "            'hidden_size': 64,\n",
    "            'n_layers': 2,\n",
    "            'n_heads': 2,\n",
    "            'inner_size': 256,\n",
    "            'hidden_dropout_prob': 0.5,\n",
    "            'attn_dropout_prob': 0.5,\n",
    "        }\n",
    "        \n",
    "    elif model_type == 'Pop':\n",
    "        model_class = Pop\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "        }\n",
    "    elif model_type == 'ItemKNN':\n",
    "        model_class = ItemKNN\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    elif model_type == 'LightGCN':\n",
    "        model_class = LightGCN\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            # LightGCNÈúÄË¶ÅË¥üÈááÊ†∑\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,  # ÊØè‰∏™Ê≠£Ê†∑Êú¨ÈÖç1‰∏™Ë¥üÊ†∑Êú¨\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'embedding_size': 64,\n",
    "            'n_layers': 3,  # GCNÂ±ÇÊï∞\n",
    "            'reg_weight': 1e-5,  # Ê≠£ÂàôÂåñÁ≥ªÊï∞\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    elif model_type == 'SimpleX':\n",
    "        from recbole.model.general_recommender import SimpleX\n",
    "        model_class = SimpleX\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'embedding_size': 64,\n",
    "            'aggregator': 'mean',  # Êàñ 'user_attention', 'self_attention'\n",
    "            'gamma': 0.5,\n",
    "            'margin': 0.9,\n",
    "            'negative_weight': 0.5,\n",
    "            'reg_weight': 1e-5,\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    # Merge user-defined parameters\n",
    "    model_config.update(kwargs)\n",
    "    \n",
    "    # Create config and dataset\n",
    "    config = Config(\n",
    "        model=model_type,\n",
    "        dataset=dataset_name,\n",
    "        config_dict=model_config\n",
    "    )\n",
    "    config['dataset_save_path'] = f'{config[\"checkpoint_dir\"]}/'\n",
    "    # Create dataset\n",
    "    model_dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, model_dataset)\n",
    "    \n",
    "    print(f\"{model_type} dataset stats:\")\n",
    "    print(f\"Users: {model_dataset.user_num}\")\n",
    "    print(f\"Items: {model_dataset.item_num}\")\n",
    "    print(f\"Interactions: {model_dataset.inter_num}\")\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    model = model_class(config, model_dataset).to(config['device'])\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "    \n",
    "    print(f\"Training {model_type} model...\")\n",
    "    \n",
    "    # torch.load compatibility settings\n",
    "    original_load = torch.load\n",
    "    def safe_load(*args, **kwargs):\n",
    "        kwargs['weights_only'] = False\n",
    "        return original_load(*args, **kwargs)\n",
    "    torch.load = safe_load\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        best_valid_score, best_valid_result = trainer.fit(\n",
    "            train_data, valid_data, saved=True, show_progress=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{model_type} training completed!\")\n",
    "        print(f\"Best validation result: {best_valid_result}\")\n",
    "        \n",
    "        # Test model\n",
    "        test_result = trainer.evaluate(test_data, load_best_model=True, show_progress=True)\n",
    "        print(f\"{model_type} test result: {test_result}\")\n",
    "        \n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'model': model,\n",
    "            'trainer': trainer,\n",
    "            'config': config,\n",
    "            'dataset': model_dataset,\n",
    "            'train_data': train_data,\n",
    "            'valid_data': valid_data,\n",
    "            'test_data': test_data,\n",
    "            'best_valid_result': best_valid_result,\n",
    "            'test_result': test_result\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        # Restore original torch.load function\n",
    "        torch.load = original_load\n",
    "\n",
    "print(\"Unified training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training All Models with Unified Function ===\n",
      "\n",
      "=== Training BPR Model ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR dataset stats:\n",
      "Users: 6041\n",
      "Items: 3417\n",
      "Interactions: 999611\n",
      "Training BPR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mTrain     0\u001b[0m:   0%|                                                          | 0/483 [00:00<?, ?it/s]\u001b[0m/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/recbole/trainer/trainer.py:235: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=self.enable_scaler)\n",
      "\u001b[1;35mTrain     0\u001b[0m:   0%|                         | 2/483 [00:00<02:34,  3.12it/s, \u001b[1;33mGPU RAM: 0.03 G/79.14 G\u001b[0m]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Train all models using unified function\n",
    "print(\"=== Training All Models with Unified Function ===\")\n",
    "\n",
    "# Store all model results\n",
    "model_results = {}\n",
    "dataset_name = \"ml-1m\"\n",
    "# Train all three models\n",
    "models_to_train = [\n",
    "    {'model_type': 'BPR', 'epochs': 100},\n",
    "    {'model_type': 'LightGCN', 'epochs': 100},\n",
    "    {'model_type': 'SimpleX', 'epochs': 100},\n",
    "    {'model_type': 'SASRec', 'epochs': 100},\n",
    "]\n",
    "\n",
    "for model_config in models_to_train:\n",
    "        result = train_model(\n",
    "            dataset_name=dataset_name,\n",
    "            model_type=model_config['model_type'],\n",
    "            epochs=model_config['epochs']\n",
    "        )\n",
    "        model_results[model_config['model_type']] = result\n",
    "        print(f\"‚úÖ {model_config['model_type']} training successful\")\n",
    "\n",
    "print(f\"\\nTraining completed! Successfully trained {len([r for r in model_results.values() if r is not None])} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.utils import load_data_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified model performance comparison and analysis\n",
    "def compare_models(model_results):\n",
    "    \"\"\"Compare all trained models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"                Model Performance Comparison Report\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract test results\n",
    "    results_summary = {}\n",
    "    for model_type, result in model_results.items():\n",
    "        if result is not None:\n",
    "            results_summary[model_type] = result['test_result']\n",
    "        else:\n",
    "            results_summary[model_type] = {}\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"\\n{'Metric':<15}\", end=\"\")\n",
    "    model_names = list(results_summary.keys())\n",
    "    for name in model_names:\n",
    "        print(f\" | {name:<12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (15 + 15 * len(model_names)))\n",
    "    \n",
    "    metrics_to_compare = ['recall@10', 'ndcg@10', 'recall@20', 'ndcg@20']\n",
    "    best_scores = {}\n",
    "    \n",
    "    for metric in metrics_to_compare:\n",
    "        print(f\"{metric:<15}\", end=\"\")\n",
    "        metric_values = []\n",
    "        \n",
    "        for model_type in model_names:\n",
    "            value = results_summary[model_type].get(metric, 'N/A')\n",
    "            if value != 'N/A':\n",
    "                print(f\" | {value:<12.4f}\", end=\"\")\n",
    "                metric_values.append((model_type, value))\n",
    "            else:\n",
    "                print(f\" | {'N/A':<12}\", end=\"\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Find best model\n",
    "        if metric_values:\n",
    "            best_model, best_score = max(metric_values, key=lambda x: x[1])\n",
    "            best_scores[metric] = (best_model, best_score)\n",
    "    \n",
    "    print(\"-\" * (15 + 15 * len(model_names)))\n",
    "    \n",
    "    # Analyze best models\n",
    "    print(f\"\\nüèÜ Best model for each metric:\")\n",
    "    for metric, (best_model, best_score) in best_scores.items():\n",
    "        print(f\"  {metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Overall model ranking\n",
    "    model_scores = {name: [] for name in model_names}\n",
    "    for metric in metrics_to_compare:\n",
    "        for model_type in model_names:\n",
    "            value = results_summary[model_type].get(metric, 0)\n",
    "            if value != 'N/A' and value != 0:\n",
    "                model_scores[model_type].append(value)\n",
    "    \n",
    "    avg_scores = {name: sum(scores)/len(scores) if scores else 0 \n",
    "                  for name, scores in model_scores.items()}\n",
    "    \n",
    "    print(f\"\\nüìä Overall model ranking:\")\n",
    "    sorted_models = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (model, score) in enumerate(sorted_models, 1):\n",
    "        print(f\"  {i}. {model}: {score:.4f} (avg score)\")\n",
    "    \n",
    "    # Model characteristics analysis\n",
    "    print(f\"\\nüìù Model characteristics:\")\n",
    "    model_analysis = {\n",
    "        'Pop': 'Item popularity based, fast training, good for cold start',\n",
    "        'BPR': 'Collaborative filtering, personalized, balanced performance',  \n",
    "        'SASRec': 'Sequential recommendation, temporal patterns, rich historical data'\n",
    "    }\n",
    "    \n",
    "    for model_type in model_names:\n",
    "        if model_type in model_analysis:\n",
    "            status = \"‚úÖ Success\" if model_results[model_type] else \"‚ùå Failed\"\n",
    "            print(f\"  ‚Ä¢ {model_type}: {model_analysis[model_type]} [{status}]\")\n",
    "    \n",
    "    return best_scores, sorted_models\n",
    "\n",
    "# Execute model comparison\n",
    "if model_results:\n",
    "    best_scores, model_ranking = compare_models(model_results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No model results to compare, please run model training first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe7bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ec8cb0",
   "metadata": {},
   "source": [
    "## Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "domain = 'Amazon_All_Beauty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# domain = 'All_Beauty'\n",
    "# meta_data = json.load(open(f'seq_rec_results/dataset/processed/{domain}/{domain}.data_maps'))\n",
    "datasets = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    f\"5core_timestamp_w_his_{domain}\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "raw_review_path = f'dataset/{domain}/{domain}.jsonl'\n",
    "if not os.path.exists(raw_review_path):\n",
    "    print(f'Downloading {domain} reviews from Hugging Face...')\n",
    "    import wget\n",
    "    wget.download(f'https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{domain}.jsonl?download=true', raw_review_path)\n",
    "# len(meta_data['item2id']), len(meta_data['user2id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34decd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "review_list = []\n",
    "with open(raw_review_path, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        review_list.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set(meta_data['user2id'].keys())\n",
    "item_set = set(meta_data['item2id'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "user2reviews = defaultdict(list)\n",
    "for review in tqdm(review_list):\n",
    "    if review['user_id'] in user_set and review['asin'] in item_set:\n",
    "        user2reviews[review['user_id']].append({\n",
    "            'asin': review['asin'],\n",
    "            'rating': review['rating'],\n",
    "            'title': review['title'],\n",
    "            'text': review['text'],\n",
    "            'item_id': meta_data['item2id'][review['asin']],\n",
    "            'timestamp': review['timestamp'],\n",
    "            'helpful_vote': review['helpful_vote'],\n",
    "            'verified_purchase': review['verified_purchase'],\n",
    "        })\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f962d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save user2reviews\n",
    "with open(f'seq_rec_results/dataset/processed/{domain}/{domain}.reviews', 'w') as f:\n",
    "    json.dump(user2reviews, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ce8ee",
   "metadata": {},
   "source": [
    "## General Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = outlines.from_transformers(\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\"),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11107f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class Rating(Enum):\n",
    "    poor = 1\n",
    "    fair = 2\n",
    "    good = 3\n",
    "    excellent = 4\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    rating: Rating\n",
    "    pros: list[str]\n",
    "    cons: list[str]\n",
    "    summary: str\n",
    "\n",
    "review = model(\n",
    "    \"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\",\n",
    "    ProductReview,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "review = ProductReview.model_validate_json(review)\n",
    "print(f\"Rating: {review.rating.name}\")  # \"Rating: good\"\n",
    "print(f\"Pros: {review.pros}\")           # \"Pros: ['great battery life', 'stunning display']\"\n",
    "print(f\"Summary: {review.summary}\")     # \"Summary: Good laptop with great display but thermal issues\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bb23",
   "metadata": {},
   "source": [
    "## vLLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "# ÂÆö‰πâ JSON schema via Pydantic Ê®°Âûã\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "json_schema = Person.model_json_schema()\n",
    "\n",
    "def main():\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-3B-Instruct\", max_model_len=100)\n",
    "\n",
    "    # ‰ΩøÁî® regex Âº∫Âà∂ËæìÂá∫ÂΩ¢ÂºèÔºå‰æãÂ¶Ç email Ê†ºÂºè\n",
    "    guided_regex = r'\"\\s*email\"\\s*:\\s*\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"'\n",
    "    guided_decoding_params_regex = GuidedDecodingParams(regex=guided_regex)\n",
    "    sampling_params_regex = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params_regex,\n",
    "        max_tokens=50\n",
    "    )\n",
    "\n",
    "    prompt_regex = (\n",
    "        \"Generate a JSON object with fields name, age, and email about a scientist:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"name\": \"Ada Lovelace\",\\n'\n",
    "        '  \"age\": 36,\\n'\n",
    "        '  \"email\": \"ada.lovelace@example.com\"\\n'\n",
    "        \"}\"\n",
    "    )\n",
    "\n",
    "    out_regex = llm.generate(prompts=prompt_regex, sampling_params=sampling_params_regex)\n",
    "    print(\"Regex-constrained output:\")\n",
    "    print(out_regex[0].outputs[0].text)\n",
    "\n",
    "    # ‰ΩøÁî® JSON schema Âº∫Âà∂ËæìÂá∫Êï¥‰∏™ÁªìÊûÑÁ¨¶Âêà Person Ê®°Âûã\n",
    "    guided_decoding_params_json = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params_json = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params_json,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    prompt_json = (\n",
    "        \"Generate a JSON object about a historical scientist with name, age (integer), and email.\"\n",
    "    )\n",
    "\n",
    "    out_json = llm.generate(prompts=prompt_json, sampling_params=sampling_params_json)\n",
    "    print(\"JSON-schema-constrained output:\")\n",
    "    print(out_json[0].outputs[0].text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362a389",
   "metadata": {},
   "source": [
    "## Draw Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c62a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "recallers = ['bpr', 'sasrec', 'fpmc', 'pop', 'itemknn']\n",
    "similarity_dict = {\n",
    "    \"ml-1m\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_sasrec\": 0.34273530362884574,\n",
    "            \"bpr_vs_fpmc\": 0.46105757444633155,\n",
    "            \"bpr_vs_pop\": 0.444434263661674,\n",
    "            \"bpr_vs_itemknn\": 0.6134421115995813,\n",
    "            \"sasrec_vs_fpmc\": 0.38874815022721565,\n",
    "            \"sasrec_vs_pop\": 0.2539883657580121,\n",
    "            \"sasrec_vs_itemknn\": 0.338885181548484,\n",
    "            \"fpmc_vs_pop\": 0.31859217811453044,\n",
    "            \"fpmc_vs_itemknn\": 0.40275755777363903,\n",
    "            \"pop_vs_itemknn\": 0.47005992461972157,\n",
    "            \"average\": 0.40347006113780354\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_sasrec\": 0.08258759583764952,\n",
    "            \"bpr_vs_fpmc\": 0.13135964670604944,\n",
    "            \"bpr_vs_pop\": 0.2577950441433609,\n",
    "            \"bpr_vs_itemknn\": 0.34590342620220366,\n",
    "            \"sasrec_vs_fpmc\": 0.19305723421020202,\n",
    "            \"sasrec_vs_pop\": 0.03979166691424811,\n",
    "            \"sasrec_vs_itemknn\": 0.0992836502099173,\n",
    "            \"fpmc_vs_pop\": 0.08083980497851037,\n",
    "            \"fpmc_vs_itemknn\": 0.08839267603482477,\n",
    "            \"pop_vs_itemknn\": 0.1641174113709889,\n",
    "            \"average\": 0.1483128156607955\n",
    "        }\n",
    "    },\n",
    "    \"steam\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_itemknn\": 0.6308220748941968,\n",
    "            \"bpr_vs_fpmc\": 0.5746376430231429,\n",
    "            \"bpr_vs_pop\": 0.854984443673129,\n",
    "            \"bpr_vs_sasrec\": 0.5508810983313444,\n",
    "            \"itemknn_vs_fpmc\": 0.5039287084961207,\n",
    "            \"itemknn_vs_pop\": 0.6003215868790582,\n",
    "            \"itemknn_vs_sasrec\": 0.4712966038266744,\n",
    "            \"fpmc_vs_pop\": 0.5317308405072102,\n",
    "            \"fpmc_vs_sasrec\": 0.4909579379310822,\n",
    "            \"pop_vs_sasrec\": 0.5320244418094473,\n",
    "            \"average\": 0.5741585379371407\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_itemknn\": 0.5976655030859763,\n",
    "            \"bpr_vs_fpmc\": 0.5909183916013645,\n",
    "            \"bpr_vs_pop\": 0.3731503703535082,\n",
    "            \"bpr_vs_sasrec\": 0.42053214798331184,\n",
    "            \"itemknn_vs_fpmc\": 0.48833146395151233,\n",
    "            \"itemknn_vs_pop\": 0.2561764282236138,\n",
    "            \"itemknn_vs_sasrec\": 0.3593697550375676,\n",
    "            \"fpmc_vs_pop\": 0.2526088409611869,\n",
    "            \"fpmc_vs_sasrec\": 0.4397719959901045,\n",
    "            \"pop_vs_sasrec\": 0.21928076296399454,\n",
    "            \"average\": 0.39978056601521406\n",
    "        }\n",
    "    },\n",
    "    \"music\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_itemknn\": 0.05931017970127901,\n",
    "            \"bpr_vs_fpmc\": 0.19183792768378685,\n",
    "            \"bpr_vs_pop\": 0.3486857576508974,\n",
    "            \"bpr_vs_sasrec\": 0.2763329019130139,\n",
    "            \"itemknn_vs_fpmc\": 0.04500210245874656,\n",
    "            \"itemknn_vs_pop\": 0.05856352597831993,\n",
    "            \"itemknn_vs_sasrec\": 0.05318460250694704,\n",
    "            \"fpmc_vs_pop\": 0.11867892641235088,\n",
    "            \"fpmc_vs_sasrec\": 0.17417130349015045,\n",
    "            \"pop_vs_sasrec\": 0.18694072081894209,\n",
    "            \"average\": 0.15127079486144343\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_itemknn\": 0.04553230390159534,\n",
    "            \"bpr_vs_fpmc\": 0.19411234240110908,\n",
    "            \"bpr_vs_pop\": 0.29511417000419926,\n",
    "            \"bpr_vs_sasrec\": 0.2600795002482322,\n",
    "            \"itemknn_vs_fpmc\": 0.03326785313655516,\n",
    "            \"itemknn_vs_pop\": 0.022283683544130312,\n",
    "            \"itemknn_vs_sasrec\": 0.03514783197819881,\n",
    "            \"fpmc_vs_pop\": 0.13908358114051256,\n",
    "            \"fpmc_vs_sasrec\": 0.1597916601956278,\n",
    "            \"pop_vs_sasrec\": 0.1450037198043686,\n",
    "            \"average\": 0.1329416646354529\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for dataset_name in similarity_dict.keys():\n",
    "    print(f'========== {dataset_name} ==========' )\n",
    "    for similarity_type in similarity_dict[dataset_name].keys():\n",
    "\n",
    "        data = np.zeros((len(recallers), len(recallers)))\n",
    "        recaller2idx = {recaller: i for i, recaller in enumerate(recallers)}\n",
    "        idx2recaller = {i: recaller for i, recaller in enumerate(recallers)}\n",
    "        for recaller1 in recallers:\n",
    "            for recaller2 in recallers:\n",
    "                if recaller1 == recaller2:\n",
    "                    data[recaller2idx[recaller1], recaller2idx[recaller2]] = 1.0\n",
    "                else:\n",
    "                    if f\"{recaller1}_vs_{recaller2}\" not in similarity_dict[dataset_name][similarity_type]:\n",
    "                        data[recaller2idx[recaller1], recaller2idx[recaller2]] = similarity_dict[dataset_name][similarity_type][f\"{recaller2}_vs_{recaller1}\"]\n",
    "                    else:\n",
    "                        data[recaller2idx[recaller1], recaller2idx[recaller2]] = similarity_dict[dataset_name][similarity_type][f\"{recaller1}_vs_{recaller2}\"]\n",
    "\n",
    "        # ËÆæÁΩÆÊ†áÁ≠æÔºàÊ®™Á∫µÂùêÊ†á 1-9Ôºâ\n",
    "        labels = [idx2recaller[i] for i in range(len(recallers))]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            data,\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            cmap=\"Blues\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar=True,\n",
    "            square=True\n",
    "        )\n",
    "        print(similarity_type)\n",
    "        print(idx2recaller)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb3c66",
   "metadata": {},
   "source": [
    "## General Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "output_dir = \"GRPO/grpo_models\"\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(last_checkpoint, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91710be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "model = outlines.from_transformers(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# Simple classification\n",
    "sentiment = model(\n",
    "    \"Analyze: 'This product completely changed my life!'\",\n",
    "    Literal[\"Positive\", \"Negative\", \"Neutral\"]\n",
    ")\n",
    "print(sentiment)  # \"Positive\"\n",
    "\n",
    "# Extract specific types\n",
    "temperature = model(\"What's the boiling point of water in Celsius?\", int)\n",
    "print(temperature)  # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class Rating(Enum):\n",
    "    poor = 1\n",
    "    fair = 2\n",
    "    good = 3\n",
    "    excellent = 4\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    rating: Rating\n",
    "    pros: list[str]\n",
    "    cons: list[str]\n",
    "    summary: str\n",
    "\n",
    "review = model(\n",
    "    [\"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\", \n",
    "    \"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\",],\n",
    "    ProductReview,\n",
    "    max_new_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750abf4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e862fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d11589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b59f93",
   "metadata": {},
   "source": [
    "=== Loading steam ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 11.929930284768995\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 74.07410124724872\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 302889\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.70832615116169%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 9.929930284768995\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 61.701174743024964\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 252111\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.75722398071744%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 9.252551020408163\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 25389\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97555108522212%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 10.027251184834123\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 25389\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97555108522212%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "=== Loading ml-10m ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 117.03065287764632\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 826.2922734627832\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 8170378\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 98.81657420789803%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 115.03065287764632\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 812.1713187702265\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 8030750\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 98.83679841007076%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 11.552871090517955\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 69814\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98988789891364%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 11.064025356576861\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 69814\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98988789891364%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "=== Loading Amazon_Toys_and_Games ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 7.992821474023939\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 20.03031629561689\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2882683\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99444662967173%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 5.992821474023939\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 15.07564449528486\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2161365\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99583621915433%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.597631896577522\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 360659\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99930520525871%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.7043477367734514\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 360659\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99930520525871%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "=== Loading ml-1m ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 137.42149718449818\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 250.9074690051406\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 829751\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 95.84647093369118%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 135.42149718449818\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 247.25582098578772\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 817675\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 95.90692041432422%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.244492208490059\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6038\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96977525968347%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3940415964024733\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6038\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96977525968347%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "=== Loading yelp2022 ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 14.340557096625059\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 33.38252505549203\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2977788\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98392394059113%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 12.340557096625059\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 28.747470214723236\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2562492\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98616598171972%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.85118142364331\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 207648\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9988789794357%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.849539311469939\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 207648\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9988789794357%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "=== Loading Amazon_Books ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 11.156473338592908\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 17.11140175856539\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 7692790\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99751849880246%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 9.156473338592908\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 14.076404805008337\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6313718\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99796335285664%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 2.8250062478746982\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 689536\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9997775729729%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0~\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 2.968235724586212\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 689536\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9997775729729%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "=== Loading book-crossing ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "Error loading book-crossing: [timestamp] is not exist in interaction [The batch_size of interaction: 116923\n",
    "    user_id, torch.Size([116923]), cpu, torch.int64\n",
    "    item_id, torch.Size([116923]), cpu, torch.int64\n",
    "    rating, torch.Size([116923]), cpu, torch.float32\n",
    "\n",
    "].\n",
    "=== Loading Amazon_All_Beauty ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 8.535353535353535\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 6.101083032490974\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 1690\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 96.97777141937445%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 6.5353535353535355\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 4.864661654135339\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 1294\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 97.68593858974587%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.5114503816793894\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 198\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.64591641481428%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.81651376146789\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 198\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.64591641481428%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "=== Loading Amazon_Musical_Instruments ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 7.8265742059315215\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 17.713158017656127\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 379221\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96345188811027%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 5.8265742059315215\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 13.218850962213795\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 282315\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97279137967531%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3051159618008183\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 48453\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99533025421748%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3866638708324595\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 48453\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99533025421748%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "=== Loading anime ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "Error loading anime: [timestamp] is not exist in interaction [The batch_size of interaction: 7793926\n",
    "    user_id, torch.Size([7793926]), cpu, torch.int64\n",
    "    item_id, torch.Size([7793926]), cpu, torch.int64\n",
    "    rating, torch.Size([7793926]), cpu, torch.float32\n",
    "\n",
    "].\n",
    "=== Loading Food ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 30.48594087959625\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 12.863031409232642\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 507408\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.92272330829917%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 28.48594087959625\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 12.019469654717842\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 474120\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9277929692295%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.7695088241547947\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 16644\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99746516953483%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.801103776647549\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 16644\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99746516953483%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_grpo.py\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "dataset = load_dataset(\"trl-lib/ultrafeedback-prompt\", split=\"train\")\n",
    "\n",
    "# Dummy reward function for demonstration purposes\n",
    "def reward_num_unique_letters(completions, **kwargs):\n",
    "    \"\"\"Reward function that rewards completions with more unique letters.\"\"\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [float(len(set(content))) for content in completion_contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba81ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\")\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    reward_funcs=reward_num_unique_letters,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ca78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Remove u\"\" wrappers from steam.item file\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_u_quotes(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove u\" at the beginning and \" at the end\n",
    "    # Pattern explanation:\n",
    "    # \\bu\" - matches u\" at word boundary (not part of another word)\n",
    "    # ([^\"]*) - captures everything inside the quotes\n",
    "    # \" - matches the closing quote\n",
    "    fixed_content = re.sub(r'\\bu\"([^\"]*)\"', r'\\1', content)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(fixed_content)\n",
    "    \n",
    "    print(f\"Fixed file written to: {output_file}\")\n",
    "    \n",
    "    # Count fixes\n",
    "    original_count = len(re.findall(r'\\bu\"[^\"]*\"', content))\n",
    "    print(f\"Removed {original_count} instances of u\\\"...\\\" wrappers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import shutil\n",
    "    \n",
    "    input_file = \"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item\"\n",
    "    output_file = \"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item.fixed\"\n",
    "    \n",
    "    # Process the file\n",
    "    remove_u_quotes(input_file, output_file)\n",
    "    \n",
    "    # Backup and replace\n",
    "    backup_file = input_file + \".backup2\"\n",
    "    shutil.copy(input_file, backup_file)\n",
    "    print(f\"Created backup at: {backup_file}\")\n",
    "    \n",
    "    shutil.move(output_file, input_file)\n",
    "    print(\"Replaced original file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca109acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tmp = pd.read_csv(\"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09831b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.columns = tmp.columns.str.split(':').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nan in tmp['product_id']\n",
    "tmp[tmp['product_id'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b0afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83307a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import json\n",
    "\n",
    "with open(\"completions_ml-1m.pkl\", \"rb\") as f:\n",
    "    completions = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01557699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset=load_dataset(\"trl-lib/Capybara\", split=\"train\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Ëá™Âª∫Êï∞ÊçÆ\n",
    "samples = [\n",
    "    {\"text\": \"User: ËØ∑Ëß£Èáä‰∏Ä‰∏ãÈáèÂ≠êËÆ°ÁÆó„ÄÇ\\nAssistant: ÈáèÂ≠êËÆ°ÁÆóÊòØÂà©Áî®ÈáèÂ≠êÂè†Âä†ÂíåÁ∫†Áº†Á≠âÂéüÁêÜËøõË°å‰ø°ÊÅØÂ§ÑÁêÜÁöÑËÆ°ÁÆóÊñπÂºè„ÄÇ\"},\n",
    "    {\"text\": \"User: ÁªôÊàë‰∏â‰∏™ Python ÊèêÈ´òÊïàÁéáÁöÑÂ∞èÊäÄÂ∑ß„ÄÇ\\nAssistant: 1) ‰ΩøÁî®ÂàóË°®Êé®ÂØºÂºèÔºõ2) ‰ΩøÁî®ÁîüÊàêÂô®Ôºõ3) ÂñÑÁî®Ê†áÂáÜÂ∫ì„ÄÇ\"},\n",
    "]\n",
    "dataset = Dataset.from_list(samples)\n",
    "\n",
    "# Ê®°Âûã\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ‚úÖ ÊóßÁâàÊú¨ÂÖºÂÆπÂÜôÊ≥ïÔºöÂè™ÊîæÊ†áÂáÜ TrainingArguments\n",
    "config = SFTConfig(\n",
    "    output_dir=\"./sft_text_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# ‚úÖ Ëøô‰∫õÂèÇÊï∞ÊîπÂú® SFTTrainer Èáå‰º†\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,  # Âú®ËøôÈáå‰º†\n",
    "    packing=False,        # Âú®ËøôÈáå‰º†\n",
    "    args=config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(\"./sft_text_output/final\")\n",
    "tokenizer.save_pretrained(\"./sft_text_output/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049c7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "rl_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_do_test_rl_use_vllm.pkl\", \"rb\"))\n",
    "sft_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_do_test_sft_use_vllm.pkl\", \"rb\"))\n",
    "ori_sft_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_use_vllm_do_sft.pkl\", \"rb\"))\n",
    "raw_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_use_vllm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5041f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-shot: {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    }\n",
      "}\n",
      "sft (discrete): {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    }\n",
      "}\n",
      "sft (soft): {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 138,\n",
      "        \"score-weight\": 0.18437745215208146\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 114,\n",
      "        \"score-weight\": 0.15215849375865598\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 67,\n",
      "        \"score-weight\": 0.0895406298484843\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 340,\n",
      "        \"score-weight\": 0.45352988586921056\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 90,\n",
      "        \"score-weight\": 0.1203935383715678\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user = 999\n",
    "import json\n",
    "\n",
    "print('zero-shot:', json.dumps(json.loads(raw_completions[user]), indent=4))\n",
    "# print('sft (discrete)')\n",
    "print('sft (discrete):', json.dumps(json.loads(ori_sft_completions[user]), indent=4))\n",
    "print('sft (soft):', json.dumps(json.loads(sft_completions[user]), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62cc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sth_completions = pkl.load(open(\"completions/completions_ml-1m.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb653964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"bpr\": {\"top-k\": 653, \"score-weight\": 0.0}, \"itemknn\": {\"top-k\": 982, \"score-weight\": 0.0}, \"fpmc\": {\"top-k\": 982, \"score-weight\": 0.982}, \"pop\": {\"top-k\": 982, \"score-weight\": -1.0}, \"sasrec\": {\"top-k\": 653, \"score-weight\": 0.9821} }'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth_completions[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "data = Dataset.load_from_disk(\"GRPO/grpo_models/ml-1m/Llama-3.2-1B-Instruct_sft_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80bcdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 150,\n",
      "        \"score-weight\": 0.20128789985964096\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 145,\n",
      "        \"score-weight\": 0.19354841692367755\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 173,\n",
      "        \"score-weight\": 0.23116514639709734\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 141,\n",
      "        \"score-weight\": 0.18910396995780204\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 138,\n",
      "        \"score-weight\": 0.18489456686178224\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('ground truth:', json.dumps(json.loads(data[999]['completion']), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45778a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Êñ∞Â¢ûtoken‰∏™Êï∞: 2\n",
      "ÁâπÊÆätokenË°®: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'additional_special_tokens': ['[num]', '[soft]']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:05<00:00,  1.30s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Âè•Â≠ê: ‰ª∑Ê†ºÊòØ[num]Ôºå‰ΩÜË¥®Âú∞Âæà[soft]„ÄÇ\n",
      "tokenize -> ['√§¬ª¬∑√¶≈Ç¬º', '√¶ƒ∫¬Ø', '[num]', '√Ø¬ºƒÆ√§¬Ωƒ®', '√®¬¥¬®', '√•ƒæ¬∞', '√•¬æƒ™', '[soft]', '√£ƒ¢ƒ§']\n",
      "ids -> [98580, 21043, 128256, 102378, 103706, 30590, 101600, 128257, 1811]\n",
      "decode -> ‰ª∑Ê†ºÊòØ[num]Ôºå‰ΩÜË¥®Âú∞Âæà[soft]„ÄÇ\n",
      "\n",
      "Âè•Â≠ê: ËØ∑Êää[num]ÊõøÊç¢ÊàêÂÆûÈôÖÊï∞Â≠óÔºåÊää[soft]ÊõøÊç¢ÊàêÊüîËΩØÂ∫¶„ÄÇ\n",
      "tokenize -> ['√®¬Ø¬∑', '√¶ƒ¨ƒ¨', '[num]', '√¶ƒΩ¬ø', '√¶ƒØ¬¢', '√¶ƒ™ƒ≤', '√•¬Æ≈Ä√©ƒªƒß', '√¶ƒ∑¬∞√•≈Éƒπ', '√Ø¬ºƒÆ√¶ƒ¨ƒ¨', '[soft]', '√¶ƒΩ¬ø', '√¶ƒØ¬¢', '√¶ƒ™ƒ≤', '√¶≈Åƒ∂', '√®¬Ω¬Ø', '√•¬∫¬¶', '√£ƒ¢ƒ§']\n",
      "ids -> [15225, 102178, 128256, 109913, 72234, 13153, 115827, 83687, 117424, 128257, 109913, 72234, 13153, 115289, 65372, 27479, 1811]\n",
      "decode -> ËØ∑Êää[num]ÊõøÊç¢ÊàêÂÆûÈôÖÊï∞Â≠óÔºåÊää[soft]ÊõøÊç¢ÊàêÊüîËΩØÂ∫¶„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"   # ÊàñÁî®‰Ω†Êú¨Âú∞/ÁßÅÊúâÊùÉÈáç\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# 1) Ê≥®ÂÜå‰∏§‰∏™Ëá™ÂÆö‰πâÁâπÊÆä token\n",
    "new_tokens = {\"additional_special_tokens\": [\"[num]\", \"[soft]\"]}\n",
    "num_added = tok.add_special_tokens(new_tokens)\n",
    "print(\"Êñ∞Â¢ûtoken‰∏™Êï∞:\", num_added)\n",
    "print(\"ÁâπÊÆätokenË°®:\", tok.special_tokens_map)\n",
    "\n",
    "# 2) ËΩΩÂÖ•Ê®°ÂûãÂπ∂ÂêåÊ≠•ËØçË°®Â§ßÂ∞èÔºàÂÖ≥ÈîÆÊ≠•È™§Ôºâ\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 3) È™åËØÅÔºöËøô‰∫õËØçÊòØÂê¶Ë¢´ÂΩì‰Ωú‰∏Ä‰∏™Êï¥‰ΩìÂàáÂàÜ\n",
    "sents = [\n",
    "    \"‰ª∑Ê†ºÊòØ[num]Ôºå‰ΩÜË¥®Âú∞Âæà[soft]„ÄÇ\",\n",
    "    \"ËØ∑Êää[num]ÊõøÊç¢ÊàêÂÆûÈôÖÊï∞Â≠óÔºåÊää[soft]ÊõøÊç¢ÊàêÊüîËΩØÂ∫¶„ÄÇ\",\n",
    "]\n",
    "for s in sents:\n",
    "    print(\"\\nÂè•Â≠ê:\", s)\n",
    "    print(\"tokenize ->\", tok.tokenize(s))\n",
    "    enc = tok(s, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    print(\"ids ->\", enc[\"input_ids\"].tolist()[0])\n",
    "    # ‰πüÂèØ‰ª•ÂèçËß£ÁúãÂõûÊîæ\n",
    "    print(\"decode ->\", tok.decode(enc[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenÊï∞Èáè: 5\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Available models: \n",
    "['{\\n  \"description\": \"Bayesian Personalized Ranking, a classic pairwise ranking method based on matrix factorization. It focuses on modeling user preference orderings.\",\\n  \"when_to_use\": \"Use when the task involves general recommendation based on long-term user preferences, without considering sequence order. Suitable for implicit feedback like clicks or likes.\",\\n  \"input\": \"A user-item interaction matrix or embeddings representing user and item factors.\",\\n  \"output\": \"Top-K candidate items ranked by the user\\'s overall preference.\"\\n}', '{\\n  \"description\": \"A Transformer-based sequential recommendation model (Self-Attentive Sequential Recommendation). It captures the order and short-term interest patterns from the user\\'s recent interactions.\",\\n  \"when_to_use\": \"Use when the task requires modeling the sequence of recent user interactions or short-term preferences. For example, predicting the next item a user might click or watch.\",\\n  \"input\": \"A chronologically ordered sequence of user-item interactions.\",\\n  \"output\": \"Top-K candidate items predicted as the next likely interactions.\"\\n}', '{\\n  \"description\": \"Factorizing Personalized Markov Chains, a hybrid model combining matrix factorization (long-term user preferences) with first-order Markov chains (short-term sequential patterns). It predicts the next item by considering both user embedding and the transition from the last interacted item.\",\\n  \"when_to_use\": \"Use when the recommendation task involves next-item prediction or session-based recommendation, where both long-term preferences and recent sequential behavior matter.\",\\n  \"input\": \"User embedding (long-term preference) and the last interacted item (short-term context).\",\\n  \"output\": \"Top-K candidate items predicted as the user\\'s next likely interaction.\"\\n}', '{\\n  \"description\": \"A simple non-personalized baseline that recommends items purely based on their overall popularity (e.g., number of interactions).\",\\n  \"when_to_use\": \"Use as a baseline for comparison or in cold-start situations where user-specific data is not available.\",\\n  \"input\": \"Global item interaction counts or frequencies.\",\\n  \"output\": \"Top-K items ranked by overall popularity.\"\\n}', '{\\n  \"description\": \"An item-based collaborative filtering model that recommends items similar to those a user has already interacted with, using item-to-item similarity (e.g., cosine similarity, Jaccard).\",\\n  \"when_to_use\": \"Use when item similarity can effectively capture user preference patterns. Works well in scenarios like e-commerce or content platforms where co-purchase or co-view signals are strong.\",\\n  \"input\": \"Item-item similarity matrix built from historical user-item interactions.\",\\n  \"output\": \"Top-K items most similar to the user\\\\u2019s past interacted items.\"\\n}']\n",
    "User Profile:\n",
    "{\n",
    "    \"purchased item numbers\": 36,\n",
    "    \"purchase history\": [\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.3,\n",
    "            \"rating_number\": 535,\n",
    "            \"price\": 7.49,\n",
    "            \"rating\": 3.0,\n",
    "            \"timestamp\": 1562688988349.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 3.8,\n",
    "            \"rating_number\": 38,\n",
    "            \"price\": 9.99,\n",
    "            \"rating\": 5.0,\n",
    "            \"timestamp\": 1570227921988.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.2,\n",
    "            \"rating_number\": 63,\n",
    "            \"price\": NaN,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580159586262.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.1,\n",
    "            \"rating_number\": 81,\n",
    "            \"price\": NaN,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580344556353.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.1,\n",
    "            \"rating_number\": 213,\n",
    "            \"price\": 15.97,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580931456931.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "Expected output format example:\n",
    "{\n",
    "  \"bpr\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"sasrec\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"fpmc\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"pop\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"itemknn\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Please output the JSON file containing the usage of ALL availablemodels.Your JSON response:\n",
    "'''\n",
    "tokens = tok(text, padding=False, return_tensors=None)\n",
    "actual_length = len(tokens[\"input_ids\"])\n",
    "print(f\"TokenÊï∞Èáè: {actual_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c07649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-21 13:05:23'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.fromtimestamp(1545397523659.0 / 1000).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73611ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
