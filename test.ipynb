{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5babfa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.general_recommender import BPR, Pop, ItemKNN, LightGCN\n",
    "from recbole.model.sequential_recommender import SASRec\n",
    "from recbole.utils import init_seed, init_logger, get_trainer\n",
    "\n",
    "# Set torch.load compatibility\n",
    "torch.serialization.add_safe_globals([dict, list, tuple, set])\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26543fb4",
   "metadata": {},
   "source": [
    "## Recaller Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon Reviews 2023 dataset using RecBole\n",
    "from GRPO.data import get_base_config_dict\n",
    "import os\n",
    "for dataset_name in os.listdir('dataset'):\n",
    "    try:\n",
    "        config_5core = Config(\n",
    "            model='SASRec',\n",
    "            dataset=dataset_name, \n",
    "            config_dict=get_base_config_dict(dataset_name)\n",
    "        )\n",
    "\n",
    "        # åˆ›å»º5-coreè¿‡æ»¤åŽçš„æ•°æ®é›†\n",
    "        print(f\"=== Loading {dataset_name} ===\")\n",
    "        print(\"Creating 5-core filtered dataset...\")\n",
    "        from recbole.utils import init_seed as recbole_init_seed\n",
    "        recbole_init_seed(seed=42, reproducibility=True)\n",
    "        dataset_5core = create_dataset(config_5core)\n",
    "        train_data_5core, valid_data_5core, test_data_5core = data_preparation(config_5core, dataset_5core)\n",
    "        print(f\"\\nðŸ“Š 5-core Filtered Dataset Statistics:\")\n",
    "        print(dataset_5core)\n",
    "        # train set stats\n",
    "        import numpy as np\n",
    "        print(train_data_5core.dataset)\n",
    "        print(np.unique(train_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "        print(valid_data_5core.dataset)\n",
    "        print(np.unique(valid_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "        print(test_data_5core.dataset)\n",
    "        print(np.unique(test_data_5core.dataset.inter_feat['user_id'].numpy()).shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {e}\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22db084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified training function defined!\n"
     ]
    }
   ],
   "source": [
    "# Define unified model training function\n",
    "def train_model(model_type, dataset_name='All_Beauty', epochs=10, **kwargs):\n",
    "    \"\"\"\n",
    "    Unified function to train recommendation models\n",
    "    \n",
    "    Args:\n",
    "        model_type: Model type ('BPR', 'SASRec', 'Pop')\n",
    "        dataset_name: Dataset name\n",
    "        epochs: Training epochsÂ·\n",
    "        **kwargs: Additional model-specific parameters\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing model, trainer, config and results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Training {model_type} Model ===\")\n",
    "    \n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        # 'data_path': 'seq_rec_results/dataset/processed/',\n",
    "        # 'benchmark_filename': ['train', 'valid', 'test'],\n",
    "        'epochs': epochs,\n",
    "        'stopping_step': 10,\n",
    "        'eval_step': 1,\n",
    "        'metrics': ['Recall', 'NDCG'],\n",
    "        'topk': [10, 50],\n",
    "        'valid_metric': 'NDCG@10',\n",
    "        'checkpoint_dir': './checkpoint',\n",
    "        'show_progress': True,\n",
    "        'save_dataset': True,\n",
    "        'save_dataloaders': True,\n",
    "    }\n",
    "    base_config.update({\n",
    "        'data_path': 'dataset',\n",
    "        'load_col': {\n",
    "            # 'inter': ['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "            'inter': ['user_id', 'item_id', 'timestamp']\n",
    "        },\n",
    "        'user_inter_num_interval': \"[5,inf)\",\n",
    "        'item_inter_num_interval': \"[5,inf)\",\n",
    "        'train_neg_sample_args': None,\n",
    "        'loss_type': 'CE',\n",
    "        # 'val_interval': {\n",
    "        #     'rating': '[3,inf)'  # åªä¿ç•™rating >= 4çš„äº¤äº’\n",
    "        # },\n",
    "        'eval_args': {\n",
    "            'split': {'RS': [0.8, 0.1, 0.1]},  # Leave-One-Out\n",
    "            'order': 'TO',  # Temporal Order\n",
    "            'group_by': 'user'\n",
    "        },\n",
    "        'ITEM_ID_FIELD': 'item_id',\n",
    "    })\n",
    "    \n",
    "    # Model-specific configurations\n",
    "    if model_type == 'BPR':\n",
    "        model_class = BPR\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,\n",
    "                'alpha': 1.0,\n",
    "                'dynamic': False,\n",
    "                'candidate_num': 0\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "        \n",
    "    elif model_type == 'SASRec':\n",
    "        model_class = SASRec\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "            'loss_type': 'CE',\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 256,\n",
    "            'max_seq_length': 50,\n",
    "            'hidden_size': 64,\n",
    "            'n_layers': 2,\n",
    "            'n_heads': 2,\n",
    "            'inner_size': 256,\n",
    "            'hidden_dropout_prob': 0.5,\n",
    "            'attn_dropout_prob': 0.5,\n",
    "        }\n",
    "        \n",
    "    elif model_type == 'Pop':\n",
    "        model_class = Pop\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "        }\n",
    "    elif model_type == 'ItemKNN':\n",
    "        model_class = ItemKNN\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': None,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    elif model_type == 'LightGCN':\n",
    "        model_class = LightGCN\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            # LightGCNéœ€è¦è´Ÿé‡‡æ ·\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,  # æ¯ä¸ªæ­£æ ·æœ¬é…1ä¸ªè´Ÿæ ·æœ¬\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'embedding_size': 64,\n",
    "            'n_layers': 3,  # GCNå±‚æ•°\n",
    "            'reg_weight': 1e-5,  # æ­£åˆ™åŒ–ç³»æ•°\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    elif model_type == 'SimpleX':\n",
    "        from recbole.model.general_recommender import SimpleX\n",
    "        model_class = SimpleX\n",
    "        model_config = {\n",
    "            **base_config,\n",
    "            'train_neg_sample_args': {\n",
    "                'distribution': 'uniform',\n",
    "                'sample_num': 1,\n",
    "            },\n",
    "            'loss_type': 'BPR',\n",
    "            'embedding_size': 64,\n",
    "            'aggregator': 'mean',  # æˆ– 'user_attention', 'self_attention'\n",
    "            'gamma': 0.5,\n",
    "            'margin': 0.9,\n",
    "            'negative_weight': 0.5,\n",
    "            'reg_weight': 1e-5,\n",
    "            'learning_rate': 0.001,\n",
    "            'train_batch_size': 2048,\n",
    "            'eval_batch_size': 2048 * 20000,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    # Merge user-defined parameters\n",
    "    model_config.update(kwargs)\n",
    "    \n",
    "    # Create config and dataset\n",
    "    config = Config(\n",
    "        model=model_type,\n",
    "        dataset=dataset_name,\n",
    "        config_dict=model_config\n",
    "    )\n",
    "    config['dataset_save_path'] = f'{config[\"checkpoint_dir\"]}/'\n",
    "    # Create dataset\n",
    "    model_dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, model_dataset)\n",
    "    \n",
    "    print(f\"{model_type} dataset stats:\")\n",
    "    print(f\"Users: {model_dataset.user_num}\")\n",
    "    print(f\"Items: {model_dataset.item_num}\")\n",
    "    print(f\"Interactions: {model_dataset.inter_num}\")\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    model = model_class(config, model_dataset).to(config['device'])\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "    \n",
    "    print(f\"Training {model_type} model...\")\n",
    "    \n",
    "    # torch.load compatibility settings\n",
    "    original_load = torch.load\n",
    "    def safe_load(*args, **kwargs):\n",
    "        kwargs['weights_only'] = False\n",
    "        return original_load(*args, **kwargs)\n",
    "    torch.load = safe_load\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        best_valid_score, best_valid_result = trainer.fit(\n",
    "            train_data, valid_data, saved=True, show_progress=True\n",
    "        )\n",
    "        \n",
    "        print(f\"{model_type} training completed!\")\n",
    "        print(f\"Best validation result: {best_valid_result}\")\n",
    "        \n",
    "        # Test model\n",
    "        test_result = trainer.evaluate(test_data, load_best_model=True, show_progress=True)\n",
    "        print(f\"{model_type} test result: {test_result}\")\n",
    "        \n",
    "        return {\n",
    "            'model_type': model_type,\n",
    "            'model': model,\n",
    "            'trainer': trainer,\n",
    "            'config': config,\n",
    "            'dataset': model_dataset,\n",
    "            'train_data': train_data,\n",
    "            'valid_data': valid_data,\n",
    "            'test_data': test_data,\n",
    "            'best_valid_result': best_valid_result,\n",
    "            'test_result': test_result\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        # Restore original torch.load function\n",
    "        torch.load = original_load\n",
    "\n",
    "print(\"Unified training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training All Models with Unified Function ===\n",
      "\n",
      "=== Training BPR Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/recbole/data/dataset/dataset.py:649: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=0, inplace=True)\n",
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/recbole/data/dataset/dataset.py:651: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m models_to_train \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBPR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m},\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# {'model_type': 'LightGCN', 'epochs': 100},\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# {'model_type': 'SimpleX', 'epochs': 100},\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# {'model_type': 'SASRec', 'epochs': 100},\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_config \u001b[38;5;129;01min\u001b[39;00m models_to_train:\n\u001b[0;32m---> 16\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         model_results[model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 156\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_type, dataset_name, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m model_dataset \u001b[38;5;241m=\u001b[39m create_dataset(config)\n\u001b[1;32m    154\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m data_preparation(config, model_dataset)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dataset\u001b[38;5;241m.\u001b[39muser_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItems: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dataset\u001b[38;5;241m.\u001b[39mitem_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 156\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_type, dataset_name, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m model_dataset \u001b[38;5;241m=\u001b[39m create_dataset(config)\n\u001b[1;32m    154\u001b[0m train_data, valid_data, test_data \u001b[38;5;241m=\u001b[39m data_preparation(config, model_dataset)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dataset\u001b[38;5;241m.\u001b[39muser_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItems: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dataset\u001b[38;5;241m.\u001b[39mitem_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/rs/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2185\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2188\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2190\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2193\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rs/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2254\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2255\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2257\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2258\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/rs/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/rs/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train all models using unified function\n",
    "print(\"=== Training All Models with Unified Function ===\")\n",
    "\n",
    "# Store all model results\n",
    "model_results = {}\n",
    "dataset_name = \"ml-1m\"\n",
    "# Train all three models\n",
    "models_to_train = [\n",
    "    # {'model_type': 'BPR', 'epochs': 100},\n",
    "    # {'model_type': 'LightGCN', 'epochs': 100},\n",
    "    # {'model_type': 'SimpleX', 'epochs': 100},\n",
    "    {'model_type': 'SASRec', 'epochs': 100},\n",
    "]\n",
    "\n",
    "for model_config in models_to_train:\n",
    "        result = train_model(\n",
    "            dataset_name=dataset_name,\n",
    "            model_type=model_config['model_type'],\n",
    "            epochs=model_config['epochs']\n",
    "        )\n",
    "        model_results[model_config['model_type']] = result\n",
    "        print(f\"âœ… {model_config['model_type']} training successful\")\n",
    "\n",
    "print(f\"\\nTraining completed! Successfully trained {len([r for r in model_results.values() if r is not None])} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcf2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.utils import load_data_and_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified model performance comparison and analysis\n",
    "def compare_models(model_results):\n",
    "    \"\"\"Compare all trained models\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"                Model Performance Comparison Report\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract test results\n",
    "    results_summary = {}\n",
    "    for model_type, result in model_results.items():\n",
    "        if result is not None:\n",
    "            results_summary[model_type] = result['test_result']\n",
    "        else:\n",
    "            results_summary[model_type] = {}\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"\\n{'Metric':<15}\", end=\"\")\n",
    "    model_names = list(results_summary.keys())\n",
    "    for name in model_names:\n",
    "        print(f\" | {name:<12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (15 + 15 * len(model_names)))\n",
    "    \n",
    "    metrics_to_compare = ['recall@10', 'ndcg@10', 'recall@20', 'ndcg@20']\n",
    "    best_scores = {}\n",
    "    \n",
    "    for metric in metrics_to_compare:\n",
    "        print(f\"{metric:<15}\", end=\"\")\n",
    "        metric_values = []\n",
    "        \n",
    "        for model_type in model_names:\n",
    "            value = results_summary[model_type].get(metric, 'N/A')\n",
    "            if value != 'N/A':\n",
    "                print(f\" | {value:<12.4f}\", end=\"\")\n",
    "                metric_values.append((model_type, value))\n",
    "            else:\n",
    "                print(f\" | {'N/A':<12}\", end=\"\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Find best model\n",
    "        if metric_values:\n",
    "            best_model, best_score = max(metric_values, key=lambda x: x[1])\n",
    "            best_scores[metric] = (best_model, best_score)\n",
    "    \n",
    "    print(\"-\" * (15 + 15 * len(model_names)))\n",
    "    \n",
    "    # Analyze best models\n",
    "    print(f\"\\nðŸ† Best model for each metric:\")\n",
    "    for metric, (best_model, best_score) in best_scores.items():\n",
    "        print(f\"  {metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Overall model ranking\n",
    "    model_scores = {name: [] for name in model_names}\n",
    "    for metric in metrics_to_compare:\n",
    "        for model_type in model_names:\n",
    "            value = results_summary[model_type].get(metric, 0)\n",
    "            if value != 'N/A' and value != 0:\n",
    "                model_scores[model_type].append(value)\n",
    "    \n",
    "    avg_scores = {name: sum(scores)/len(scores) if scores else 0 \n",
    "                  for name, scores in model_scores.items()}\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall model ranking:\")\n",
    "    sorted_models = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (model, score) in enumerate(sorted_models, 1):\n",
    "        print(f\"  {i}. {model}: {score:.4f} (avg score)\")\n",
    "    \n",
    "    # Model characteristics analysis\n",
    "    print(f\"\\nðŸ“ Model characteristics:\")\n",
    "    model_analysis = {\n",
    "        'Pop': 'Item popularity based, fast training, good for cold start',\n",
    "        'BPR': 'Collaborative filtering, personalized, balanced performance',  \n",
    "        'SASRec': 'Sequential recommendation, temporal patterns, rich historical data'\n",
    "    }\n",
    "    \n",
    "    for model_type in model_names:\n",
    "        if model_type in model_analysis:\n",
    "            status = \"âœ… Success\" if model_results[model_type] else \"âŒ Failed\"\n",
    "            print(f\"  â€¢ {model_type}: {model_analysis[model_type]} [{status}]\")\n",
    "    \n",
    "    return best_scores, sorted_models\n",
    "\n",
    "# Execute model comparison\n",
    "if model_results:\n",
    "    best_scores, model_ranking = compare_models(model_results)\n",
    "else:\n",
    "    print(\"âš ï¸ No model results to compare, please run model training first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe7bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ec8cb0",
   "metadata": {},
   "source": [
    "## Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "domain = 'Amazon_All_Beauty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# domain = 'All_Beauty'\n",
    "# meta_data = json.load(open(f'seq_rec_results/dataset/processed/{domain}/{domain}.data_maps'))\n",
    "datasets = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    f\"5core_timestamp_w_his_{domain}\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "raw_review_path = f'dataset/{domain}/{domain}.jsonl'\n",
    "if not os.path.exists(raw_review_path):\n",
    "    print(f'Downloading {domain} reviews from Hugging Face...')\n",
    "    import wget\n",
    "    wget.download(f'https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{domain}.jsonl?download=true', raw_review_path)\n",
    "# len(meta_data['item2id']), len(meta_data['user2id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34decd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "review_list = []\n",
    "with open(raw_review_path, 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        review_list.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = set(meta_data['user2id'].keys())\n",
    "item_set = set(meta_data['item2id'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "user2reviews = defaultdict(list)\n",
    "for review in tqdm(review_list):\n",
    "    if review['user_id'] in user_set and review['asin'] in item_set:\n",
    "        user2reviews[review['user_id']].append({\n",
    "            'asin': review['asin'],\n",
    "            'rating': review['rating'],\n",
    "            'title': review['title'],\n",
    "            'text': review['text'],\n",
    "            'item_id': meta_data['item2id'][review['asin']],\n",
    "            'timestamp': review['timestamp'],\n",
    "            'helpful_vote': review['helpful_vote'],\n",
    "            'verified_purchase': review['verified_purchase'],\n",
    "        })\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f962d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save user2reviews\n",
    "with open(f'seq_rec_results/dataset/processed/{domain}/{domain}.reviews', 'w') as f:\n",
    "    json.dump(user2reviews, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ce8ee",
   "metadata": {},
   "source": [
    "## General Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = outlines.from_transformers(\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\"),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11107f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class Rating(Enum):\n",
    "    poor = 1\n",
    "    fair = 2\n",
    "    good = 3\n",
    "    excellent = 4\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    rating: Rating\n",
    "    pros: list[str]\n",
    "    cons: list[str]\n",
    "    summary: str\n",
    "\n",
    "review = model(\n",
    "    \"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\",\n",
    "    ProductReview,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "review = ProductReview.model_validate_json(review)\n",
    "print(f\"Rating: {review.rating.name}\")  # \"Rating: good\"\n",
    "print(f\"Pros: {review.pros}\")           # \"Pros: ['great battery life', 'stunning display']\"\n",
    "print(f\"Summary: {review.summary}\")     # \"Summary: Good laptop with great display but thermal issues\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206bb23",
   "metadata": {},
   "source": [
    "## vLLM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "# å®šä¹‰ JSON schema via Pydantic æ¨¡åž‹\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "json_schema = Person.model_json_schema()\n",
    "\n",
    "def main():\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-3B-Instruct\", max_model_len=100)\n",
    "\n",
    "    # ä½¿ç”¨ regex å¼ºåˆ¶è¾“å‡ºå½¢å¼ï¼Œä¾‹å¦‚ email æ ¼å¼\n",
    "    guided_regex = r'\"\\s*email\"\\s*:\\s*\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"'\n",
    "    guided_decoding_params_regex = GuidedDecodingParams(regex=guided_regex)\n",
    "    sampling_params_regex = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params_regex,\n",
    "        max_tokens=50\n",
    "    )\n",
    "\n",
    "    prompt_regex = (\n",
    "        \"Generate a JSON object with fields name, age, and email about a scientist:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"name\": \"Ada Lovelace\",\\n'\n",
    "        '  \"age\": 36,\\n'\n",
    "        '  \"email\": \"ada.lovelace@example.com\"\\n'\n",
    "        \"}\"\n",
    "    )\n",
    "\n",
    "    out_regex = llm.generate(prompts=prompt_regex, sampling_params=sampling_params_regex)\n",
    "    print(\"Regex-constrained output:\")\n",
    "    print(out_regex[0].outputs[0].text)\n",
    "\n",
    "    # ä½¿ç”¨ JSON schema å¼ºåˆ¶è¾“å‡ºæ•´ä¸ªç»“æž„ç¬¦åˆ Person æ¨¡åž‹\n",
    "    guided_decoding_params_json = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params_json = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params_json,\n",
    "        max_tokens=100\n",
    "    )\n",
    "\n",
    "    prompt_json = (\n",
    "        \"Generate a JSON object about a historical scientist with name, age (integer), and email.\"\n",
    "    )\n",
    "\n",
    "    out_json = llm.generate(prompts=prompt_json, sampling_params=sampling_params_json)\n",
    "    print(\"JSON-schema-constrained output:\")\n",
    "    print(out_json[0].outputs[0].text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362a389",
   "metadata": {},
   "source": [
    "## Draw Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c62a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "recallers = ['bpr', 'sasrec', 'fpmc', 'pop', 'itemknn']\n",
    "similarity_dict = {\n",
    "    \"ml-1m\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_sasrec\": 0.34273530362884574,\n",
    "            \"bpr_vs_fpmc\": 0.46105757444633155,\n",
    "            \"bpr_vs_pop\": 0.444434263661674,\n",
    "            \"bpr_vs_itemknn\": 0.6134421115995813,\n",
    "            \"sasrec_vs_fpmc\": 0.38874815022721565,\n",
    "            \"sasrec_vs_pop\": 0.2539883657580121,\n",
    "            \"sasrec_vs_itemknn\": 0.338885181548484,\n",
    "            \"fpmc_vs_pop\": 0.31859217811453044,\n",
    "            \"fpmc_vs_itemknn\": 0.40275755777363903,\n",
    "            \"pop_vs_itemknn\": 0.47005992461972157,\n",
    "            \"average\": 0.40347006113780354\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_sasrec\": 0.08258759583764952,\n",
    "            \"bpr_vs_fpmc\": 0.13135964670604944,\n",
    "            \"bpr_vs_pop\": 0.2577950441433609,\n",
    "            \"bpr_vs_itemknn\": 0.34590342620220366,\n",
    "            \"sasrec_vs_fpmc\": 0.19305723421020202,\n",
    "            \"sasrec_vs_pop\": 0.03979166691424811,\n",
    "            \"sasrec_vs_itemknn\": 0.0992836502099173,\n",
    "            \"fpmc_vs_pop\": 0.08083980497851037,\n",
    "            \"fpmc_vs_itemknn\": 0.08839267603482477,\n",
    "            \"pop_vs_itemknn\": 0.1641174113709889,\n",
    "            \"average\": 0.1483128156607955\n",
    "        }\n",
    "    },\n",
    "    \"steam\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_itemknn\": 0.6308220748941968,\n",
    "            \"bpr_vs_fpmc\": 0.5746376430231429,\n",
    "            \"bpr_vs_pop\": 0.854984443673129,\n",
    "            \"bpr_vs_sasrec\": 0.5508810983313444,\n",
    "            \"itemknn_vs_fpmc\": 0.5039287084961207,\n",
    "            \"itemknn_vs_pop\": 0.6003215868790582,\n",
    "            \"itemknn_vs_sasrec\": 0.4712966038266744,\n",
    "            \"fpmc_vs_pop\": 0.5317308405072102,\n",
    "            \"fpmc_vs_sasrec\": 0.4909579379310822,\n",
    "            \"pop_vs_sasrec\": 0.5320244418094473,\n",
    "            \"average\": 0.5741585379371407\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_itemknn\": 0.5976655030859763,\n",
    "            \"bpr_vs_fpmc\": 0.5909183916013645,\n",
    "            \"bpr_vs_pop\": 0.3731503703535082,\n",
    "            \"bpr_vs_sasrec\": 0.42053214798331184,\n",
    "            \"itemknn_vs_fpmc\": 0.48833146395151233,\n",
    "            \"itemknn_vs_pop\": 0.2561764282236138,\n",
    "            \"itemknn_vs_sasrec\": 0.3593697550375676,\n",
    "            \"fpmc_vs_pop\": 0.2526088409611869,\n",
    "            \"fpmc_vs_sasrec\": 0.4397719959901045,\n",
    "            \"pop_vs_sasrec\": 0.21928076296399454,\n",
    "            \"average\": 0.39978056601521406\n",
    "        }\n",
    "    },\n",
    "    \"music\": {\n",
    "        \"jaccard\": {\n",
    "            \"bpr_vs_itemknn\": 0.05931017970127901,\n",
    "            \"bpr_vs_fpmc\": 0.19183792768378685,\n",
    "            \"bpr_vs_pop\": 0.3486857576508974,\n",
    "            \"bpr_vs_sasrec\": 0.2763329019130139,\n",
    "            \"itemknn_vs_fpmc\": 0.04500210245874656,\n",
    "            \"itemknn_vs_pop\": 0.05856352597831993,\n",
    "            \"itemknn_vs_sasrec\": 0.05318460250694704,\n",
    "            \"fpmc_vs_pop\": 0.11867892641235088,\n",
    "            \"fpmc_vs_sasrec\": 0.17417130349015045,\n",
    "            \"pop_vs_sasrec\": 0.18694072081894209,\n",
    "            \"average\": 0.15127079486144343\n",
    "        },\n",
    "        \"rbo\": {\n",
    "            \"bpr_vs_itemknn\": 0.04553230390159534,\n",
    "            \"bpr_vs_fpmc\": 0.19411234240110908,\n",
    "            \"bpr_vs_pop\": 0.29511417000419926,\n",
    "            \"bpr_vs_sasrec\": 0.2600795002482322,\n",
    "            \"itemknn_vs_fpmc\": 0.03326785313655516,\n",
    "            \"itemknn_vs_pop\": 0.022283683544130312,\n",
    "            \"itemknn_vs_sasrec\": 0.03514783197819881,\n",
    "            \"fpmc_vs_pop\": 0.13908358114051256,\n",
    "            \"fpmc_vs_sasrec\": 0.1597916601956278,\n",
    "            \"pop_vs_sasrec\": 0.1450037198043686,\n",
    "            \"average\": 0.1329416646354529\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for dataset_name in similarity_dict.keys():\n",
    "    print(f'========== {dataset_name} ==========' )\n",
    "    for similarity_type in similarity_dict[dataset_name].keys():\n",
    "\n",
    "        data = np.zeros((len(recallers), len(recallers)))\n",
    "        recaller2idx = {recaller: i for i, recaller in enumerate(recallers)}\n",
    "        idx2recaller = {i: recaller for i, recaller in enumerate(recallers)}\n",
    "        for recaller1 in recallers:\n",
    "            for recaller2 in recallers:\n",
    "                if recaller1 == recaller2:\n",
    "                    data[recaller2idx[recaller1], recaller2idx[recaller2]] = 1.0\n",
    "                else:\n",
    "                    if f\"{recaller1}_vs_{recaller2}\" not in similarity_dict[dataset_name][similarity_type]:\n",
    "                        data[recaller2idx[recaller1], recaller2idx[recaller2]] = similarity_dict[dataset_name][similarity_type][f\"{recaller2}_vs_{recaller1}\"]\n",
    "                    else:\n",
    "                        data[recaller2idx[recaller1], recaller2idx[recaller2]] = similarity_dict[dataset_name][similarity_type][f\"{recaller1}_vs_{recaller2}\"]\n",
    "\n",
    "        # è®¾ç½®æ ‡ç­¾ï¼ˆæ¨ªçºµåæ ‡ 1-9ï¼‰\n",
    "        labels = [idx2recaller[i] for i in range(len(recallers))]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            data,\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            cmap=\"Blues\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar=True,\n",
    "            square=True\n",
    "        )\n",
    "        print(similarity_type)\n",
    "        print(idx2recaller)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb3c66",
   "metadata": {},
   "source": [
    "## General Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "output_dir = \"GRPO/grpo_models\"\n",
    "last_checkpoint = get_last_checkpoint(output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(last_checkpoint, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91710be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "model = outlines.from_transformers(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# Simple classification\n",
    "sentiment = model(\n",
    "    \"Analyze: 'This product completely changed my life!'\",\n",
    "    Literal[\"Positive\", \"Negative\", \"Neutral\"]\n",
    ")\n",
    "print(sentiment)  # \"Positive\"\n",
    "\n",
    "# Extract specific types\n",
    "temperature = model(\"What's the boiling point of water in Celsius?\", int)\n",
    "print(temperature)  # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "class Rating(Enum):\n",
    "    poor = 1\n",
    "    fair = 2\n",
    "    good = 3\n",
    "    excellent = 4\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    rating: Rating\n",
    "    pros: list[str]\n",
    "    cons: list[str]\n",
    "    summary: str\n",
    "\n",
    "review = model(\n",
    "    [\"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\", \n",
    "    \"Review: The XPS 13 has great battery life and a stunning display, but it runs hot and the webcam is poor quality.\",],\n",
    "    ProductReview,\n",
    "    max_new_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750abf4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e862fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d11589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b59f93",
   "metadata": {},
   "source": [
    "=== Loading steam ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 11.929930284768995\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 74.07410124724872\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 302889\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.70832615116169%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 9.929930284768995\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 61.701174743024964\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 252111\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.75722398071744%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 9.252551020408163\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 25389\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97555108522212%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "\u001b[1;35msteam\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 25390\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 4090\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 10.027251184834123\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 25389\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97555108522212%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'product_id', 'timestamp', 'product_id_list', 'timestamp_list', 'item_length']\n",
    "(25389,)\n",
    "=== Loading ml-10m ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 117.03065287764632\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 826.2922734627832\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 8170378\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 98.81657420789803%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 115.03065287764632\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 812.1713187702265\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 8030750\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 98.83679841007076%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 11.552871090517955\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 69814\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98988789891364%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "\u001b[1;35mml-10m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 69815\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 9889\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 11.064025356576861\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 69814\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98988789891364%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(69814,)\n",
    "=== Loading Amazon_Toys_and_Games ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 7.992821474023939\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 20.03031629561689\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2882683\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99444662967173%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 5.992821474023939\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 15.07564449528486\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2161365\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99583621915433%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.597631896577522\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 360659\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99930520525871%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "\u001b[1;35mAmazon_Toys_and_Games\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 360660\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 143927\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.7043477367734514\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 360659\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99930520525871%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(360659,)\n",
    "=== Loading ml-1m ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 137.42149718449818\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 250.9074690051406\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 829751\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 95.84647093369118%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 135.42149718449818\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 247.25582098578772\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 817675\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 95.90692041432422%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.244492208490059\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6038\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96977525968347%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "\u001b[1;35mml-1m\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 6039\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 3308\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3940415964024733\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6038\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96977525968347%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(6038,)\n",
    "=== Loading yelp2022 ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 14.340557096625059\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 33.38252505549203\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2977788\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98392394059113%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 12.340557096625059\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 28.747470214723236\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 2562492\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.98616598171972%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.85118142364331\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 207648\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9988789794357%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "\u001b[1;35myelp2022\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 207649\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 89204\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.849539311469939\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 207648\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9988789794357%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(207648,)\n",
    "=== Loading Amazon_Books ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 11.156473338592908\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 17.11140175856539\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 7692790\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99751849880246%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 9.156473338592908\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 14.076404805008337\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 6313718\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99796335285664%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 2.8250062478746982\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 689536\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9997775729729%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "\u001b[1;35mAmazon_Books\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 689537\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0~\n",
    "\u001b[1;34mThe number of items\u001b[0m: 449585\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 2.968235724586212\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 689536\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9997775729729%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(689536,)\n",
    "=== Loading book-crossing ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "Error loading book-crossing: [timestamp] is not exist in interaction [The batch_size of interaction: 116923\n",
    "    user_id, torch.Size([116923]), cpu, torch.int64\n",
    "    item_id, torch.Size([116923]), cpu, torch.int64\n",
    "    rating, torch.Size([116923]), cpu, torch.float32\n",
    "\n",
    "].\n",
    "=== Loading Amazon_All_Beauty ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 8.535353535353535\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 6.101083032490974\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 1690\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 96.97777141937445%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 6.5353535353535355\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 4.864661654135339\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 1294\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 97.68593858974587%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.5114503816793894\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 198\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.64591641481428%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "\u001b[1;35mAmazon_All_Beauty\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 199\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 281\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.81651376146789\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 198\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.64591641481428%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(198,)\n",
    "=== Loading Amazon_Musical_Instruments ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 7.8265742059315215\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 17.713158017656127\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 379221\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.96345188811027%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 5.8265742059315215\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 13.218850962213795\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 282315\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.97279137967531%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3051159618008183\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 48453\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99533025421748%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "\u001b[1;35mAmazon_Musical_Instruments\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 48454\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 21414\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 3.3866638708324595\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 48453\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99533025421748%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(48453,)\n",
    "=== Loading anime ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "Error loading anime: [timestamp] is not exist in interaction [The batch_size of interaction: 7793926\n",
    "    user_id, torch.Size([7793926]), cpu, torch.int64\n",
    "    item_id, torch.Size([7793926]), cpu, torch.int64\n",
    "    rating, torch.Size([7793926]), cpu, torch.float32\n",
    "\n",
    "].\n",
    "=== Loading Food ===\n",
    "Creating 5-core filtered dataset...\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:648: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=0, inplace=True)\n",
    "/home/zzheng3/miniconda3/envs/rs/lib/python3.12/site-packages/recbole/data/dataset/dataset.py:650: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
    "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
    "\n",
    "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
    "\n",
    "\n",
    "  feat[field].fillna(value=feat[field].mean(), inplace=True)\n",
    "\n",
    ":bar_chart: 5-core Filtered Dataset Statistics:\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 30.48594087959625\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 12.863031409232642\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 507408\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.92272330829917%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 28.48594087959625\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 12.019469654717842\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 474120\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.9277929692295%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.7695088241547947\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 16644\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99746516953483%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)\n",
    "\u001b[1;35mFood\u001b[0m\n",
    "\u001b[1;34mThe number of users\u001b[0m: 16645\n",
    "\u001b[1;34mAverage actions of users\u001b[0m: 1.0\n",
    "\u001b[1;34mThe number of items\u001b[0m: 39448\n",
    "\u001b[1;34mAverage actions of items\u001b[0m: 1.801103776647549\n",
    "\u001b[1;34mThe number of inters\u001b[0m: 16644\n",
    "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.99746516953483%\n",
    "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'rating', 'timestamp', 'item_id_list', 'rating_list', 'timestamp_list', 'item_length']\n",
    "(16644,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a0f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_grpo.py\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "dataset = load_dataset(\"trl-lib/ultrafeedback-prompt\", split=\"train\")\n",
    "\n",
    "# Dummy reward function for demonstration purposes\n",
    "def reward_num_unique_letters(completions, **kwargs):\n",
    "    \"\"\"Reward function that rewards completions with more unique letters.\"\"\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [float(len(set(content))) for content in completion_contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba81ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(output_dir=\"Qwen2-0.5B-GRPO\")\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    reward_funcs=reward_num_unique_letters,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ca78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Remove u\"\" wrappers from steam.item file\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_u_quotes(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove u\" at the beginning and \" at the end\n",
    "    # Pattern explanation:\n",
    "    # \\bu\" - matches u\" at word boundary (not part of another word)\n",
    "    # ([^\"]*) - captures everything inside the quotes\n",
    "    # \" - matches the closing quote\n",
    "    fixed_content = re.sub(r'\\bu\"([^\"]*)\"', r'\\1', content)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(fixed_content)\n",
    "    \n",
    "    print(f\"Fixed file written to: {output_file}\")\n",
    "    \n",
    "    # Count fixes\n",
    "    original_count = len(re.findall(r'\\bu\"[^\"]*\"', content))\n",
    "    print(f\"Removed {original_count} instances of u\\\"...\\\" wrappers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import shutil\n",
    "    \n",
    "    input_file = \"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item\"\n",
    "    output_file = \"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item.fixed\"\n",
    "    \n",
    "    # Process the file\n",
    "    remove_u_quotes(input_file, output_file)\n",
    "    \n",
    "    # Backup and replace\n",
    "    backup_file = input_file + \".backup2\"\n",
    "    shutil.copy(input_file, backup_file)\n",
    "    print(f\"Created backup at: {backup_file}\")\n",
    "    \n",
    "    shutil.move(output_file, input_file)\n",
    "    print(\"Replaced original file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca109acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tmp = pd.read_csv(\"/home/zzheng3/AmazonReviews2023/dataset/steam/steam.item\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09831b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.columns = tmp.columns.str.split(':').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nan in tmp['product_id']\n",
    "tmp[tmp['product_id'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b0afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83307a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import json\n",
    "\n",
    "with open(\"completions_ml-1m.pkl\", \"rb\") as f:\n",
    "    completions = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01557699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset=load_dataset(\"trl-lib/Capybara\", split=\"train\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# è‡ªå»ºæ•°æ®\n",
    "samples = [\n",
    "    {\"text\": \"User: è¯·è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—ã€‚\\nAssistant: é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­å åŠ å’Œçº ç¼ ç­‰åŽŸç†è¿›è¡Œä¿¡æ¯å¤„ç†çš„è®¡ç®—æ–¹å¼ã€‚\"},\n",
    "    {\"text\": \"User: ç»™æˆ‘ä¸‰ä¸ª Python æé«˜æ•ˆçŽ‡çš„å°æŠ€å·§ã€‚\\nAssistant: 1) ä½¿ç”¨åˆ—è¡¨æŽ¨å¯¼å¼ï¼›2) ä½¿ç”¨ç”Ÿæˆå™¨ï¼›3) å–„ç”¨æ ‡å‡†åº“ã€‚\"},\n",
    "]\n",
    "dataset = Dataset.from_list(samples)\n",
    "\n",
    "# æ¨¡åž‹\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# âœ… æ—§ç‰ˆæœ¬å…¼å®¹å†™æ³•ï¼šåªæ”¾æ ‡å‡† TrainingArguments\n",
    "config = SFTConfig(\n",
    "    output_dir=\"./sft_text_output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# âœ… è¿™äº›å‚æ•°æ”¹åœ¨ SFTTrainer é‡Œä¼ \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,  # åœ¨è¿™é‡Œä¼ \n",
    "    packing=False,        # åœ¨è¿™é‡Œä¼ \n",
    "    args=config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(\"./sft_text_output/final\")\n",
    "tokenizer.save_pretrained(\"./sft_text_output/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "049c7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "rl_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_do_test_rl_use_vllm.pkl\", \"rb\"))\n",
    "sft_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_do_test_sft_use_vllm.pkl\", \"rb\"))\n",
    "ori_sft_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_use_vllm_do_sft.pkl\", \"rb\"))\n",
    "raw_completions = pkl.load(open(\"completions/completions_ml-1m_use_hf_local_do_test_use_vllm.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5041f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-shot: {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 50,\n",
      "        \"score-weight\": 0.5\n",
      "    }\n",
      "}\n",
      "sft (discrete): {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 0,\n",
      "        \"score-weight\": 0.0\n",
      "    }\n",
      "}\n",
      "sft (soft): {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 138,\n",
      "        \"score-weight\": 0.18437745215208146\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 114,\n",
      "        \"score-weight\": 0.15215849375865598\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 67,\n",
      "        \"score-weight\": 0.0895406298484843\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 340,\n",
      "        \"score-weight\": 0.45352988586921056\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 90,\n",
      "        \"score-weight\": 0.1203935383715678\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user = 999\n",
    "import json\n",
    "\n",
    "print('zero-shot:', json.dumps(json.loads(raw_completions[user]), indent=4))\n",
    "# print('sft (discrete)')\n",
    "print('sft (discrete):', json.dumps(json.loads(ori_sft_completions[user]), indent=4))\n",
    "print('sft (soft):', json.dumps(json.loads(sft_completions[user]), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62cc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sth_completions = pkl.load(open(\"completions/completions_ml-1m.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb653964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"bpr\": {\"top-k\": 653, \"score-weight\": 0.0}, \"itemknn\": {\"top-k\": 982, \"score-weight\": 0.0}, \"fpmc\": {\"top-k\": 982, \"score-weight\": 0.982}, \"pop\": {\"top-k\": 982, \"score-weight\": -1.0}, \"sasrec\": {\"top-k\": 653, \"score-weight\": 0.9821} }'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth_completions[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a06bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "data = Dataset.load_from_disk(\"GRPO/grpo_models/ml-1m/Llama-3.2-1B-Instruct_sft_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80bcdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth: {\n",
      "    \"bpr\": {\n",
      "        \"top-k\": 150,\n",
      "        \"score-weight\": 0.20128789985964096\n",
      "    },\n",
      "    \"sasrec\": {\n",
      "        \"top-k\": 145,\n",
      "        \"score-weight\": 0.19354841692367755\n",
      "    },\n",
      "    \"fpmc\": {\n",
      "        \"top-k\": 173,\n",
      "        \"score-weight\": 0.23116514639709734\n",
      "    },\n",
      "    \"pop\": {\n",
      "        \"top-k\": 141,\n",
      "        \"score-weight\": 0.18910396995780204\n",
      "    },\n",
      "    \"itemknn\": {\n",
      "        \"top-k\": 138,\n",
      "        \"score-weight\": 0.18489456686178224\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('ground truth:', json.dumps(json.loads(data[999]['completion']), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45778a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–°å¢žtokenä¸ªæ•°: 2\n",
      "ç‰¹æ®Štokenè¡¨: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'additional_special_tokens': ['[num]', '[soft]']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.30s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¥å­: ä»·æ ¼æ˜¯[num]ï¼Œä½†è´¨åœ°å¾ˆ[soft]ã€‚\n",
      "tokenize -> ['Ã¤Â»Â·Ã¦Å‚Â¼', 'Ã¦ÄºÂ¯', '[num]', 'Ã¯Â¼Ä®Ã¤Â½Ä¨', 'Ã¨Â´Â¨', 'Ã¥Ä¾Â°', 'Ã¥Â¾Äª', '[soft]', 'Ã£Ä¢Ä¤']\n",
      "ids -> [98580, 21043, 128256, 102378, 103706, 30590, 101600, 128257, 1811]\n",
      "decode -> ä»·æ ¼æ˜¯[num]ï¼Œä½†è´¨åœ°å¾ˆ[soft]ã€‚\n",
      "\n",
      "å¥å­: è¯·æŠŠ[num]æ›¿æ¢æˆå®žé™…æ•°å­—ï¼ŒæŠŠ[soft]æ›¿æ¢æˆæŸ”è½¯åº¦ã€‚\n",
      "tokenize -> ['Ã¨Â¯Â·', 'Ã¦Ä¬Ä¬', '[num]', 'Ã¦Ä½Â¿', 'Ã¦Ä¯Â¢', 'Ã¦ÄªÄ²', 'Ã¥Â®Å€Ã©Ä»Ä§', 'Ã¦Ä·Â°Ã¥ÅƒÄ¹', 'Ã¯Â¼Ä®Ã¦Ä¬Ä¬', '[soft]', 'Ã¦Ä½Â¿', 'Ã¦Ä¯Â¢', 'Ã¦ÄªÄ²', 'Ã¦ÅÄ¶', 'Ã¨Â½Â¯', 'Ã¥ÂºÂ¦', 'Ã£Ä¢Ä¤']\n",
      "ids -> [15225, 102178, 128256, 109913, 72234, 13153, 115827, 83687, 117424, 128257, 109913, 72234, 13153, 115289, 65372, 27479, 1811]\n",
      "decode -> è¯·æŠŠ[num]æ›¿æ¢æˆå®žé™…æ•°å­—ï¼ŒæŠŠ[soft]æ›¿æ¢æˆæŸ”è½¯åº¦ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"   # æˆ–ç”¨ä½ æœ¬åœ°/ç§æœ‰æƒé‡\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# 1) æ³¨å†Œä¸¤ä¸ªè‡ªå®šä¹‰ç‰¹æ®Š token\n",
    "new_tokens = {\"additional_special_tokens\": [\"[num]\", \"[soft]\"]}\n",
    "num_added = tok.add_special_tokens(new_tokens)\n",
    "print(\"æ–°å¢žtokenä¸ªæ•°:\", num_added)\n",
    "print(\"ç‰¹æ®Štokenè¡¨:\", tok.special_tokens_map)\n",
    "\n",
    "# 2) è½½å…¥æ¨¡åž‹å¹¶åŒæ­¥è¯è¡¨å¤§å°ï¼ˆå…³é”®æ­¥éª¤ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 3) éªŒè¯ï¼šè¿™äº›è¯æ˜¯å¦è¢«å½“ä½œä¸€ä¸ªæ•´ä½“åˆ‡åˆ†\n",
    "sents = [\n",
    "    \"ä»·æ ¼æ˜¯[num]ï¼Œä½†è´¨åœ°å¾ˆ[soft]ã€‚\",\n",
    "    \"è¯·æŠŠ[num]æ›¿æ¢æˆå®žé™…æ•°å­—ï¼ŒæŠŠ[soft]æ›¿æ¢æˆæŸ”è½¯åº¦ã€‚\",\n",
    "]\n",
    "for s in sents:\n",
    "    print(\"\\nå¥å­:\", s)\n",
    "    print(\"tokenize ->\", tok.tokenize(s))\n",
    "    enc = tok(s, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    print(\"ids ->\", enc[\"input_ids\"].tolist()[0])\n",
    "    # ä¹Ÿå¯ä»¥åè§£çœ‹å›žæ”¾\n",
    "    print(\"decode ->\", tok.decode(enc[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987c162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenæ•°é‡: 5\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Available models: \n",
    "['{\\n  \"description\": \"Bayesian Personalized Ranking, a classic pairwise ranking method based on matrix factorization. It focuses on modeling user preference orderings.\",\\n  \"when_to_use\": \"Use when the task involves general recommendation based on long-term user preferences, without considering sequence order. Suitable for implicit feedback like clicks or likes.\",\\n  \"input\": \"A user-item interaction matrix or embeddings representing user and item factors.\",\\n  \"output\": \"Top-K candidate items ranked by the user\\'s overall preference.\"\\n}', '{\\n  \"description\": \"A Transformer-based sequential recommendation model (Self-Attentive Sequential Recommendation). It captures the order and short-term interest patterns from the user\\'s recent interactions.\",\\n  \"when_to_use\": \"Use when the task requires modeling the sequence of recent user interactions or short-term preferences. For example, predicting the next item a user might click or watch.\",\\n  \"input\": \"A chronologically ordered sequence of user-item interactions.\",\\n  \"output\": \"Top-K candidate items predicted as the next likely interactions.\"\\n}', '{\\n  \"description\": \"Factorizing Personalized Markov Chains, a hybrid model combining matrix factorization (long-term user preferences) with first-order Markov chains (short-term sequential patterns). It predicts the next item by considering both user embedding and the transition from the last interacted item.\",\\n  \"when_to_use\": \"Use when the recommendation task involves next-item prediction or session-based recommendation, where both long-term preferences and recent sequential behavior matter.\",\\n  \"input\": \"User embedding (long-term preference) and the last interacted item (short-term context).\",\\n  \"output\": \"Top-K candidate items predicted as the user\\'s next likely interaction.\"\\n}', '{\\n  \"description\": \"A simple non-personalized baseline that recommends items purely based on their overall popularity (e.g., number of interactions).\",\\n  \"when_to_use\": \"Use as a baseline for comparison or in cold-start situations where user-specific data is not available.\",\\n  \"input\": \"Global item interaction counts or frequencies.\",\\n  \"output\": \"Top-K items ranked by overall popularity.\"\\n}', '{\\n  \"description\": \"An item-based collaborative filtering model that recommends items similar to those a user has already interacted with, using item-to-item similarity (e.g., cosine similarity, Jaccard).\",\\n  \"when_to_use\": \"Use when item similarity can effectively capture user preference patterns. Works well in scenarios like e-commerce or content platforms where co-purchase or co-view signals are strong.\",\\n  \"input\": \"Item-item similarity matrix built from historical user-item interactions.\",\\n  \"output\": \"Top-K items most similar to the user\\\\u2019s past interacted items.\"\\n}']\n",
    "User Profile:\n",
    "{\n",
    "    \"purchased item numbers\": 36,\n",
    "    \"purchase history\": [\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.3,\n",
    "            \"rating_number\": 535,\n",
    "            \"price\": 7.49,\n",
    "            \"rating\": 3.0,\n",
    "            \"timestamp\": 1562688988349.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 3.8,\n",
    "            \"rating_number\": 38,\n",
    "            \"price\": 9.99,\n",
    "            \"rating\": 5.0,\n",
    "            \"timestamp\": 1570227921988.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.2,\n",
    "            \"rating_number\": 63,\n",
    "            \"price\": NaN,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580159586262.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.1,\n",
    "            \"rating_number\": 81,\n",
    "            \"price\": NaN,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580344556353.0\n",
    "        },\n",
    "        {\n",
    "            \"categories\": \"All Beauty\",\n",
    "            \"average_rating\": 4.1,\n",
    "            \"rating_number\": 213,\n",
    "            \"price\": 15.97,\n",
    "            \"rating\": 4.0,\n",
    "            \"timestamp\": 1580931456931.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "Expected output format example:\n",
    "{\n",
    "  \"bpr\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"sasrec\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"fpmc\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"pop\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  },\n",
    "  \"itemknn\": {\n",
    "    \"top-k\": \"integer between 1 and 500\",\n",
    "    \"score-weight\": \"float between 0 and 1\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Please output the JSON file containing the usage of ALL availablemodels.Your JSON response:\n",
    "'''\n",
    "tokens = tok(text, padding=False, return_tensors=None)\n",
    "actual_length = len(tokens[\"input_ids\"])\n",
    "print(f\"Tokenæ•°é‡: {actual_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c07649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-21 13:05:23'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.fromtimestamp(1545397523659.0 / 1000).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73611ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzheng3/.conda/envs/rs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at GRPO/soft_models/Amazon_All_Beauty/Llama-3.2-1B-Instruct_rl/checkpoint-792 and are newly initialized: ['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "ckpt_path = \"GRPO/soft_models/Amazon_All_Beauty/Llama-3.2-1B-Instruct_rl/checkpoint-792\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fcfa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[num]' in tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a112333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@misc{Authors14,\n",
      "\n",
      " author = {FirstName LastName},\n",
      " title = {The frobnicatable foo filter},\n",
      " note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},\n",
      " year = 2014\n",
      "}\n",
      "\n",
      "@misc{Authors14b,\n",
      "\n",
      " author = {FirstName LastName},\n",
      " title = {Frobnication tutorial},\n",
      " note = {Supplied as supplemental material {\tt tr.pdf}},\n",
      " year = 2014\n",
      "}\n",
      "\n",
      "@article{Alpher02,\n",
      "\n",
      "author = {FirstName Alpher},\n",
      "title = {Frobnication},\n",
      "journal = PAMI,\n",
      "volume = 12,\n",
      "number = 1,\n",
      "pages = {234--778},\n",
      "year = 2002\n",
      "}\n",
      "\n",
      "@article{Alpher03,\n",
      "\n",
      "author = {FirstName Alpher and  FirstName Fotheringham-Smythe},\n",
      "title = {Frobnication revisited},\n",
      "journal = {Journal of Foo},\n",
      "volume = 13,\n",
      "number = 1,\n",
      "pages = {234--778},\n",
      "year = 2003\n",
      "}\n",
      "\n",
      "@article{Alpher04,\n",
      "\n",
      "author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},\n",
      "title = {Can a machine frobnicate?},\n",
      "journal = {Journal of Foo},\n",
      "volume = 14,\n",
      "number = 1,\n",
      "pages = {234--778},\n",
      "year = 2004\n",
      "}\n",
      "\n",
      "@inproceedings{Alpher05,\n",
      "\n",
      "author = {FirstName Alpher and FirstName Gamow},\n",
      "title = {Can a computer frobnicate?},\n",
      "booktitle = CVPR,\n",
      "pages = {234--778},\n",
      "year = 2005\n",
      "}\n",
      "\n",
      "@article{zhang2025unraveling,\n",
      "\n",
      "  title={Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging},\n",
      "  author={Zhang, Haobo and Zhou, Jiayu},\n",
      "  journal={arXiv preprint arXiv:2505.22934},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{zheng2025decouple,\n",
      "\n",
      "  title={Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging},\n",
      "  author={Zheng, Shenghe and Wang, Hongzhi and Huang, Chenyu and Wang, Xiaohui and Chen, Tao and Fan, Jiayuan and Hu, Shuyue and Ye, Peng},\n",
      "  journal={arXiv preprint arXiv:2505.15875},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{miyano2025adaptive,\n",
      "\n",
      "  title={Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation},\n",
      "  author={Miyano, Ryota and Arase, Yuki},\n",
      "  journal={arXiv preprint arXiv:2505.24174},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{shao2025icm,\n",
      "\n",
      "  title={ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation},\n",
      "  author={Shao, Yihua and Lin, Xiaofeng and Long, Xinwei and Chen, Siyu and Yan, Minxi and Liu, Yang and Yan, Ziyang and Ma, Ao and Tang, Hao and Guo, Jingcai},\n",
      "  journal={arXiv preprint arXiv:2508.04153},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{liang2025thanora,\n",
      "\n",
      "  title={ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation},\n",
      "  author={Liang, Jian and Huang, Wenke and Guo, Xianda and Wan, Guancheng and Du, Bo and Ye, Mang},\n",
      "  journal={arXiv preprint arXiv:2505.18640},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{yadav2023ties,\n",
      "\n",
      "  title={Ties-merging: Resolving interference when merging models},\n",
      "  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},\n",
      "  journal={Advances in Neural Information Processing Systems},\n",
      "  volume={36},\n",
      "  pages={7093--7115},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{jang2024personalized,\n",
      "\n",
      "  title={Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging},\n",
      "  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},\n",
      "  booktitle={Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{tang2025lora,\n",
      "\n",
      "  title={Lora-null: Low-rank adaptation via null space for large language models},\n",
      "  author={Tang, Pengwei and Liu, Yong and Zhang, Dongjie and Wu, Xing and Zhang, Debing},\n",
      "  journal={arXiv preprint arXiv:2503.02659},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@inproceedings{stoica2025model,\n",
      "\n",
      "  title={Model merging with SVD to tie the Knots},\n",
      "  author={Stoica, George and Ramesh, Pratik and Ecsedi, Boglarka and Choshen, Leshem and Hoffman, Judy},\n",
      "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@inproceedings{zhao2025merging,\n",
      "\n",
      "  title={Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering},\n",
      "  author={Zhao, Ziyu and Shen, Tao and Zhu, Didi and Li, Zexi and Su, Jing and Wang, Xuwu and Wu, Fei},\n",
      "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{zhao2025each,\n",
      "\n",
      "  title={Each rank could be an expert: Single-ranked mixture of experts lora for multi-task learning},\n",
      "  author={Zhao, Ziyu and Zhou, Yixiao and Zhang, Zhi and Zhu, Didi and Shen, Tao and Li, Zexi and Yang, Jinluan and Wang, Xuwu and Su, Jing and Kuang, Kun and others},\n",
      "  journal={arXiv preprint arXiv:2501.15103},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{su2025tensorized,\n",
      "\n",
      "  title={Tensorized Clustered LoRA Merging for Multi-Task Interference},\n",
      "  author={Su, Zhan and Mo, Fengran and Liang, Guojun and Zhang, Jinghan and Wen, Bingbing and Tiwari, Prayag and Nie, Jian-Yun},\n",
      "  journal={arXiv preprint arXiv:2508.03999},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@inproceedings{huang2024lorahub,\n",
      "\n",
      "  title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition},\n",
      "  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},\n",
      "  booktitle={First Conference on Language Modeling},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@inproceedings{xiao2024lm,\n",
      "\n",
      "  title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging},\n",
      "  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Xing, Xingrun},\n",
      "  booktitle={Findings of the Association for Computational Linguistics ACL 2024},\n",
      "  pages={2474--2488},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{neyshabur2015path,\n",
      "\n",
      "  title={Path-sgd: Path-normalized optimization in deep neural networks},\n",
      "  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},\n",
      "  journal={Advances in neural information processing systems},\n",
      "  volume={28},\n",
      "  year={2015}\n",
      "}\n",
      "\n",
      "@article{badrinarayanan2015symmetry,\n",
      "\n",
      "  title={Symmetry-invariant optimization in deep networks},\n",
      "  author={Badrinarayanan, Vijay and Mishra, Bamdev and Cipolla, Roberto},\n",
      "  journal={arXiv preprint arXiv:1511.01754},\n",
      "  year={2015}\n",
      "}\n",
      "\n",
      "@article{du2018algorithmic,\n",
      "\n",
      "  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},\n",
      "  author={Du, Simon S and Hu, Wei and Lee, Jason D},\n",
      "  journal={Advances in neural information processing systems},\n",
      "  volume={31},\n",
      "  year={2018}\n",
      "}\n",
      "\n",
      "@inproceedings{meng2019g,\n",
      "\n",
      "  title={G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space},\n",
      "  author={Meng, Qi and Zheng, Shuxin and Zhang, Huishuai and Chen, Wei and Ye, Qiwei and Ma, Zhi-Ming and Yu, Nenghai and Liu, Tie-Yan},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2019}\n",
      "}\n",
      "\n",
      "@inproceedings{kunin2021neural,\n",
      "\n",
      "  title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},\n",
      "  author={Kunin, Daniel and Sagastuy-Brena, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2021}\n",
      "}\n",
      "\n",
      "@article{zhao2022symmetry,\n",
      "\n",
      "  title={Symmetry teleportation for accelerated optimization},\n",
      "  author={Zhao, Bo and Dehmamy, Nima and Walters, Robin and Yu, Rose},\n",
      "  journal={Advances in neural information processing systems},\n",
      "  volume={35},\n",
      "  pages={16679--16690},\n",
      "  year={2022}\n",
      "}\n",
      "\n",
      "@inproceedings{zhao2023symmetries,\n",
      "\n",
      "  title={Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow},\n",
      "  author={Zhao, Bo and Ganev, Iordan and Walters, Robin and Yu, Rose and Dehmamy, Nima},\n",
      "  booktitle={The Eleventh International Conference on Learning Representations},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{zhao2024improving,\n",
      "\n",
      "  title={Improving Convergence and Generalization Using Parameter Symmetries},\n",
      "  author={Zhao, Bo and Gower, Robert M and Walters, Robin and Yu, Rose},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{brea2019weight,\n",
      "\n",
      "  title={Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape},\n",
      "  author={Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},\n",
      "  journal={arXiv preprint arXiv:1907.02911},\n",
      "  year={2019}\n",
      "}\n",
      "\n",
      "@inproceedings{ainsworth2023git,\n",
      "\n",
      "  title={Git re-basin: Merging models modulo permutation symmetries},\n",
      "  author={Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddharth},\n",
      "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{entezari2022role,\n",
      "\n",
      "  title={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},\n",
      "  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2022}\n",
      "}\n",
      "\n",
      "@inproceedings{simsek2021geometry,\n",
      "\n",
      "  title={Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances},\n",
      "  author={Simsek, Berfin and Ged, Fran{\\c{c}}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Cl{'e}ment and Gerstner, Wulfram and Brea, Johanni},\n",
      "  booktitle={International Conference on Machine Learning},\n",
      "  pages={9722--9732},\n",
      "  year={2021}\n",
      "}\n",
      "\n",
      "@inproceedings{wang2020federated,\n",
      "\n",
      "  title={Federated Learning with Matched Averaging},\n",
      "  author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2020}\n",
      "}\n",
      "\n",
      "@inproceedings{navon2024equivariant,\n",
      "\n",
      "  title={Equivariant Deep Weight Space Alignment},\n",
      "  author={Navon, Aviv and Shamsian, Aviv and Fetaya, Ethan and Chechik, Gal and Dym, Nadav and Maron, Haggai},\n",
      "  booktitle={International Conference on Machine Learning},\n",
      "  pages={37376--37395},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{singh2020model,\n",
      "\n",
      "  title={Model fusion via optimal transport},\n",
      "  author={Singh, Sidak Pal and Jaggi, Martin},\n",
      "  journal={Advances in Neural Information Processing Systems},\n",
      "  volume={33},\n",
      "  pages={22045--22055},\n",
      "  year={2020}\n",
      "}\n",
      "\n",
      "@inproceedings{imfeld2024transformer,\n",
      "\n",
      "  title={Transformer Fusion with Optimal Transport},\n",
      "  author={Imfeld, Moritz and Graldi, Jacopo and Giordano, Marco and Hofmann, Thomas and Anagnostidis, Sotiris and Singh, Sidak Pal},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@inproceedings{zhang2025beyond,\n",
      "\n",
      "  title={Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion},\n",
      "  author={Zhang, Binchi and Zheng, Zaiyi and Chen, Zhengzhang and Li, Jundong},\n",
      "  booktitle={Forty-second International Conference on Machine Learning},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{zhou2023neural,\n",
      "\n",
      "  title={Neural functional transformers},\n",
      "  author={Zhou, Allan and Yang, Kaien and Jiang, Yiding and Burns, Kaylee and Xu, Winnie and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},\n",
      "  journal={Advances in neural information processing systems},\n",
      "  volume={36},\n",
      "  pages={77485--77502},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{lim2024graph,\n",
      "\n",
      "  title={Graph Metanetworks for Processing Diverse Neural Architectures},\n",
      "  author={Lim, Derek and Maron, Haggai and Law, Marc T and Lorraine, Jonathan and Lucas, James},\n",
      "  booktitle={International Conference on Learning Representations},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{zhou2023permutation,\n",
      "\n",
      "  title={Permutation equivariant neural functionals},\n",
      "  author={Zhou, Allan and Yang, Kaien and Burns, Kaylee and Cardace, Adriano and Jiang, Yiding and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},\n",
      "  journal={Advances in neural information processing systems},\n",
      "  volume={36},\n",
      "  pages={24966--24992},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{tran2025equivariant,\n",
      "\n",
      "  title={Equivariant Neural Functional Networks for Transformers},\n",
      "  author={Tran, Hoang V and Vo, Thieu and Huu, Tho Tran and Nguyen-Nhat, Minh-Khoi and Tran, Thanh and Pham, Duy-Tung and Nguyen, Tan Minh and others},\n",
      "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{ziyin2024parameter,\n",
      "\n",
      "  title={Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent},\n",
      "  author={Ziyin, Liu and Wang, Mingze and Li, Hongchao and Wu, Lei},\n",
      "  journal={arXiv preprint arXiv:2402.07193},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@inproceedings{ziyin2024symmetry,\n",
      "\n",
      "  title={Symmetry Induces Structure and Constraint of Learning},\n",
      "  author={Liu, Ziyin},\n",
      "  booktitle={International Conference on Machine Learning},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@article{naveed2025comprehensive,\n",
      "\n",
      "  title={A comprehensive overview of large language models},\n",
      "  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},\n",
      "  journal={ACM Transactions on Intelligent Systems and Technology},\n",
      "  volume={16},\n",
      "  number={5},\n",
      "  pages={1--72},\n",
      "  year={2025}\n",
      "}\n",
      "\n",
      "@article{kasneci2023chatgpt,\n",
      "\n",
      "  title={ChatGPT for good? On opportunities and challenges of large language models for education},\n",
      "  author={Kasneci, Enkelejda and Se{\\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},\n",
      "  journal={Learning and individual differences},\n",
      "  volume={103},\n",
      "  pages={102274},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@article{thirunavukarasu2023large,\n",
      "\n",
      "  title={Large language models in medicine},\n",
      "  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},\n",
      "  journal={Nature medicine},\n",
      "  volume={29},\n",
      "  number={8},\n",
      "  pages={1930--1940},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@article{han2024parameter,\n",
      "\n",
      "  title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},\n",
      "  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},\n",
      "  journal={Transactions on Machine Learning Research},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@inproceedings{hu2022lora,\n",
      "\n",
      "  title={LoRA: Low-Rank Adaptation of Large Language Models},\n",
      "  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},\n",
      "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2022},\n",
      "  url={https://openreview.net/forum?id=nZeVKeeFYf9}\n",
      "}\n",
      "\n",
      "@book{Aho:72,\n",
      "\n",
      "    author  = {Alfred V. Aho and Jeffrey D. Ullman},\n",
      "    title   = {The Theory of Parsing, Translation and Compiling},\n",
      "    year    = \"1972\",\n",
      "    volume  = \"1\",\n",
      "    publisher = {Prentice-Hall},\n",
      "    address = {Englewood Cliffs, NJ}\n",
      "}\n",
      "\n",
      "@book{APA:83,\n",
      "\n",
      "    author  = {{American Psychological Association}},\n",
      "    title   = {Publications Manual},\n",
      "    year    = \"1983\",\n",
      "    publisher = {American Psychological Association},\n",
      "    address = {Washington, DC}\n",
      "}\n",
      "\n",
      "@inproceedings{wortsman2022model,\n",
      "\n",
      "  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},\n",
      "  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},\n",
      "  booktitle={International Conference on Machine Learning},\n",
      "  pages={23965--23998},\n",
      "  year={2022},\n",
      "  organization={PMLR}\n",
      "}\n",
      "\n",
      "@article{ilharco2023editing,\n",
      "\n",
      "  title={Editing models with task arithmetic},\n",
      "  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},\n",
      "  journal={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@article{yadav2024ties,\n",
      "\n",
      "  title={TIES-Merging: Resolving Interference When Merging Models},\n",
      "  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin and Bansal, Mohit},\n",
      "  journal={Advances in Neural Information Processing Systems},\n",
      "  volume={36},\n",
      "  year={2024}\n",
      "}\n",
      "\n",
      "@inproceedings{zhang2023adalora,\n",
      "\n",
      "  title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},\n",
      "  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},\n",
      "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@inproceedings{wang2018glue,\n",
      "\n",
      "  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
      "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
      "  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},\n",
      "  pages={353--355},\n",
      "  year={2018}\n",
      "}\n",
      "\n",
      "@article{zhang2025rotation,\n",
      "\n",
      "  title={Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion},\n",
      "  author={Zhang, Binchi and Zheng, Zaiyi and Chen, Zhengzhang and Li, Jundong},\n",
      "  journal={Proceedings of the 42nd International Conference on Machine Learning},\n",
      "  year={2025},\n",
      "  organization={PMLR}\n",
      "}\n",
      "\n",
      "@article{entezari2022git,\n",
      "\n",
      "  title={The role of permutation invariance in linear mode connectivity of neural networks},\n",
      "  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},\n",
      "  journal={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2022}\n",
      "}\n",
      "\n",
      "@inproceedings{singh2020federated,\n",
      "\n",
      "  title={Model fusion via optimal transport},\n",
      "  author={Singh, Sidak Pal and Jaggi, Martin},\n",
      "  booktitle={Advances in Neural Information Processing Systems},\n",
      "  volume={33},\n",
      "  pages={22045--22055},\n",
      "  year={2020}\n",
      "}\n",
      "\n",
      "@inproceedings{matena2022merging,\n",
      "\n",
      "  title={Merging models with fisher-weighted averaging},\n",
      "  author={Matena, Michael and Raffel, Colin},\n",
      "  booktitle={Advances in Neural Information Processing Systems},\n",
      "  volume={35},\n",
      "  pages={17703--17716},\n",
      "  year={2022}\n",
      "}\n",
      "\n",
      "@article{jin2023dataless,\n",
      "\n",
      "  title={Dataless Knowledge Fusion by Merging Weights of Language Models},\n",
      "  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},\n",
      "  journal={Proceedings of the International Conference on Learning Representations},\n",
      "  year={2023}\n",
      "}\n",
      "\n",
      "@article{Chandra:81,\n",
      "\n",
      "\tauthor = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},\n",
      "\tyear = \"1981\",\n",
      "\ttitle = {Alternation},\n",
      "\tjournal = {Journal of the Association for Computing Machinery},\n",
      "\tvolume = \"28\",\n",
      "\tnumber = \"1\",\n",
      "\tpages = \"114--133\",\n",
      "\tdoi = \"10.1145/322234.322243\",\n",
      "}\n",
      "\n",
      "@inproceedings{andrew2007scalable,\n",
      "\n",
      "  title={Scalable training of {L1}-regularized log-linear models},\n",
      "  author={Andrew, Galen and Gao, Jianfeng},\n",
      "  booktitle={Proceedings of the 24th International Conference on Machine Learning},\n",
      "  pages={33--40},\n",
      "  year={2007},\n",
      "}\n",
      "\n",
      "@book{Gusfield:97,\n",
      "\n",
      "    author  = {Dan Gusfield},\n",
      "    title   = {Algorithms on Strings, Trees and Sequences},\n",
      "    year    = \"1997\",\n",
      "    publisher = {Cambridge University Press},\n",
      "    address = {Cambridge, UK}\n",
      "}\n",
      "\n",
      "@article{rasooli-tetrault-2015,\n",
      "\n",
      "    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},\n",
      "    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},\n",
      "    journal   = {Computing Research Repository},\n",
      "    volume    = {arXiv:1503.06733},\n",
      "    year      = {2015},\n",
      "    url       = {http://arxiv.org/abs/1503.06733},\n",
      "    note    = {version 2}\n",
      "}\n",
      "\n",
      "@article{Ando2005,\n",
      "\n",
      "\tAcmid = {1194905},\n",
      "\tAuthor = {Ando, Rie Kubota and Zhang, Tong},\n",
      "\tIssn = {1532-4435},\n",
      "\tIssue_Date = {12/1/2005},\n",
      "\tJournal = {Journal of Machine Learning Research},\n",
      "\tMonth = dec,\n",
      "\tNumpages = {37},\n",
      "\tPages = {1817--1853},\n",
      "\tPublisher = {JMLR.org},\n",
      "\tTitle = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},\n",
      "\tVolume = {6},\n",
      "\tYear = {2005}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_bibtex_entries(bibtex_str):\n",
    "    \"\"\"\n",
    "    Parse BibTeX entries from a string and return a dict: key -> entry_text\n",
    "    \"\"\"\n",
    "    entries = {}\n",
    "    pattern = r'@(\\w+)\\s*{\\s*([^,]+)\\s*,(.*?)\\n}'\n",
    "    matches = re.findall(pattern, bibtex_str, flags=re.DOTALL)\n",
    "\n",
    "    for entry_type, entry_key, entry_body in matches:\n",
    "        full_entry = f\"@{entry_type}{{{entry_key},\\n{entry_body}\\n}}\"\n",
    "        entries[entry_key.strip()] = full_entry.strip()\n",
    "\n",
    "    return entries\n",
    "\n",
    "def merge_bibtex(bib1, bib2):\n",
    "    \"\"\"\n",
    "    Merge two BibTeX strings, deduplicate by citation key.\n",
    "    \"\"\"\n",
    "    entries1 = parse_bibtex_entries(bib1)\n",
    "    entries2 = parse_bibtex_entries(bib2)\n",
    "\n",
    "    # Merge (entries2 overrides entries1 when conflict)\n",
    "    merged = {**entries1, **entries2}\n",
    "\n",
    "    # Convert back to BibTeX text\n",
    "    return \"\\n\\n\".join(merged.values())\n",
    "\n",
    "\n",
    "# Example usage\n",
    "bibtex_a = \"\"\"\n",
    "@misc{Authors14,\n",
    " author = {FirstName LastName},\n",
    " title = {The frobnicatable foo filter},\n",
    " note = {Face and Gesture submission ID 324. Supplied as supplemental material {\\tt fg324.pdf}},\n",
    " year = 2014\n",
    "}\n",
    "\n",
    "@misc{Authors14b,\n",
    " author = {FirstName LastName},\n",
    " title = {Frobnication tutorial},\n",
    " note = {Supplied as supplemental material {\\tt tr.pdf}},\n",
    " year = 2014\n",
    "}\n",
    "\n",
    "@article{Alpher02,\n",
    "author = {FirstName Alpher},\n",
    "title = {Frobnication},\n",
    "journal = PAMI,\n",
    "volume = 12,\n",
    "number = 1,\n",
    "pages = {234--778},\n",
    "year = 2002\n",
    "}\n",
    "\n",
    "@article{Alpher03,\n",
    "author = {FirstName Alpher and  FirstName Fotheringham-Smythe},\n",
    "title = {Frobnication revisited},\n",
    "journal = {Journal of Foo},\n",
    "volume = 13,\n",
    "number = 1,\n",
    "pages = {234--778},\n",
    "year = 2003\n",
    "}\n",
    "\n",
    "@article{Alpher04,\n",
    "author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},\n",
    "title = {Can a machine frobnicate?},\n",
    "journal = {Journal of Foo},\n",
    "volume = 14,\n",
    "number = 1,\n",
    "pages = {234--778},\n",
    "year = 2004\n",
    "}\n",
    "\n",
    "@inproceedings{Alpher05,\n",
    "author = {FirstName Alpher and FirstName Gamow},\n",
    "title = {Can a computer frobnicate?},\n",
    "booktitle = CVPR,\n",
    "pages = {234--778},\n",
    "year = 2005\n",
    "}\n",
    "\n",
    "@article{zhang2025unraveling,\n",
    "  title={Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging},\n",
    "  author={Zhang, Haobo and Zhou, Jiayu},\n",
    "  journal={arXiv preprint arXiv:2505.22934},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{zheng2025decouple,\n",
    "  title={Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging},\n",
    "  author={Zheng, Shenghe and Wang, Hongzhi and Huang, Chenyu and Wang, Xiaohui and Chen, Tao and Fan, Jiayuan and Hu, Shuyue and Ye, Peng},\n",
    "  journal={arXiv preprint arXiv:2505.15875},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{miyano2025adaptive,\n",
    "  title={Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation},\n",
    "  author={Miyano, Ryota and Arase, Yuki},\n",
    "  journal={arXiv preprint arXiv:2505.24174},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{shao2025icm,\n",
    "  title={ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation},\n",
    "  author={Shao, Yihua and Lin, Xiaofeng and Long, Xinwei and Chen, Siyu and Yan, Minxi and Liu, Yang and Yan, Ziyang and Ma, Ao and Tang, Hao and Guo, Jingcai},\n",
    "  journal={arXiv preprint arXiv:2508.04153},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{liang2025thanora,\n",
    "  title={ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation},\n",
    "  author={Liang, Jian and Huang, Wenke and Guo, Xianda and Wan, Guancheng and Du, Bo and Ye, Mang},\n",
    "  journal={arXiv preprint arXiv:2505.18640},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{yadav2023ties,\n",
    "  title={Ties-merging: Resolving interference when merging models},\n",
    "  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},\n",
    "  journal={Advances in Neural Information Processing Systems},\n",
    "  volume={36},\n",
    "  pages={7093--7115},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{jang2024personalized,\n",
    "  title={Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging},\n",
    "  author={Jang, Joel and Kim, Seungone and Lin, Bill Yuchen and Wang, Yizhong and Hessel, Jack and Zettlemoyer, Luke and Hajishirzi, Hannaneh and Choi, Yejin and Ammanabrolu, Prithviraj},\n",
    "  booktitle={Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{tang2025lora,\n",
    "  title={Lora-null: Low-rank adaptation via null space for large language models},\n",
    "  author={Tang, Pengwei and Liu, Yong and Zhang, Dongjie and Wu, Xing and Zhang, Debing},\n",
    "  journal={arXiv preprint arXiv:2503.02659},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@inproceedings{stoica2025model,\n",
    "  title={Model merging with SVD to tie the Knots},\n",
    "  author={Stoica, George and Ramesh, Pratik and Ecsedi, Boglarka and Choshen, Leshem and Hoffman, Judy},\n",
    "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@inproceedings{zhao2025merging,\n",
    "  title={Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering},\n",
    "  author={Zhao, Ziyu and Shen, Tao and Zhu, Didi and Li, Zexi and Su, Jing and Wang, Xuwu and Wu, Fei},\n",
    "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{zhao2025each,\n",
    "  title={Each rank could be an expert: Single-ranked mixture of experts lora for multi-task learning},\n",
    "  author={Zhao, Ziyu and Zhou, Yixiao and Zhang, Zhi and Zhu, Didi and Shen, Tao and Li, Zexi and Yang, Jinluan and Wang, Xuwu and Su, Jing and Kuang, Kun and others},\n",
    "  journal={arXiv preprint arXiv:2501.15103},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{su2025tensorized,\n",
    "  title={Tensorized Clustered LoRA Merging for Multi-Task Interference},\n",
    "  author={Su, Zhan and Mo, Fengran and Liang, Guojun and Zhang, Jinghan and Wen, Bingbing and Tiwari, Prayag and Nie, Jian-Yun},\n",
    "  journal={arXiv preprint arXiv:2508.03999},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@inproceedings{huang2024lorahub,\n",
    "  title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition},\n",
    "  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},\n",
    "  booktitle={First Conference on Language Modeling},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@inproceedings{xiao2024lm,\n",
    "  title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging},\n",
    "  author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Xing, Xingrun},\n",
    "  booktitle={Findings of the Association for Computational Linguistics ACL 2024},\n",
    "  pages={2474--2488},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{neyshabur2015path,\n",
    "  title={Path-sgd: Path-normalized optimization in deep neural networks},\n",
    "  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},\n",
    "  journal={Advances in neural information processing systems},\n",
    "  volume={28},\n",
    "  year={2015}\n",
    "}\n",
    "\n",
    "@article{badrinarayanan2015symmetry,\n",
    "  title={Symmetry-invariant optimization in deep networks},\n",
    "  author={Badrinarayanan, Vijay and Mishra, Bamdev and Cipolla, Roberto},\n",
    "  journal={arXiv preprint arXiv:1511.01754},\n",
    "  year={2015}\n",
    "}\n",
    "\n",
    "@article{du2018algorithmic,\n",
    "  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},\n",
    "  author={Du, Simon S and Hu, Wei and Lee, Jason D},\n",
    "  journal={Advances in neural information processing systems},\n",
    "  volume={31},\n",
    "  year={2018}\n",
    "}\n",
    "\n",
    "@inproceedings{meng2019g,\n",
    "  title={G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space},\n",
    "  author={Meng, Qi and Zheng, Shuxin and Zhang, Huishuai and Chen, Wei and Ye, Qiwei and Ma, Zhi-Ming and Yu, Nenghai and Liu, Tie-Yan},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2019}\n",
    "}\n",
    "\n",
    "@inproceedings{kunin2021neural,\n",
    "  title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},\n",
    "  author={Kunin, Daniel and Sagastuy-Brena, Javier and Ganguli, Surya and Yamins, Daniel LK and Tanaka, Hidenori},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2021}\n",
    "}\n",
    "\n",
    "@article{zhao2022symmetry,\n",
    "  title={Symmetry teleportation for accelerated optimization},\n",
    "  author={Zhao, Bo and Dehmamy, Nima and Walters, Robin and Yu, Rose},\n",
    "  journal={Advances in neural information processing systems},\n",
    "  volume={35},\n",
    "  pages={16679--16690},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@inproceedings{zhao2023symmetries,\n",
    "  title={Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow},\n",
    "  author={Zhao, Bo and Ganev, Iordan and Walters, Robin and Yu, Rose and Dehmamy, Nima},\n",
    "  booktitle={The Eleventh International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{zhao2024improving,\n",
    "  title={Improving Convergence and Generalization Using Parameter Symmetries},\n",
    "  author={Zhao, Bo and Gower, Robert M and Walters, Robin and Yu, Rose},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{brea2019weight,\n",
    "  title={Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape},\n",
    "  author={Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},\n",
    "  journal={arXiv preprint arXiv:1907.02911},\n",
    "  year={2019}\n",
    "}\n",
    "\n",
    "@inproceedings{ainsworth2023git,\n",
    "  title={Git Re-Basin: Merging Models modulo Permutation Symmetries},\n",
    "  author={Ainsworth, Samuel and Hayase, Jonathan and Srinivasa, Siddhartha},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{entezari2022role,\n",
    "  title={The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},\n",
    "  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@inproceedings{simsek2021geometry,\n",
    "  title={Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances},\n",
    "  author={Simsek, Berfin and Ged, Fran{\\c{c}}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Cl{\\'e}ment and Gerstner, Wulfram and Brea, Johanni},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  pages={9722--9732},\n",
    "  year={2021}\n",
    "}\n",
    "\n",
    "@inproceedings{wang2020federated,\n",
    "  title={Federated Learning with Matched Averaging},\n",
    "  author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2020}\n",
    "}\n",
    "\n",
    "@inproceedings{navon2024equivariant,\n",
    "  title={Equivariant Deep Weight Space Alignment},\n",
    "  author={Navon, Aviv and Shamsian, Aviv and Fetaya, Ethan and Chechik, Gal and Dym, Nadav and Maron, Haggai},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  pages={37376--37395},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{singh2020model,\n",
    "  title={Model fusion via optimal transport},\n",
    "  author={Singh, Sidak Pal and Jaggi, Martin},\n",
    "  journal={Advances in Neural Information Processing Systems},\n",
    "  volume={33},\n",
    "  pages={22045--22055},\n",
    "  year={2020}\n",
    "}\n",
    "\n",
    "@inproceedings{imfeld2024transformer,\n",
    "  title={Transformer Fusion with Optimal Transport},\n",
    "  author={Imfeld, Moritz and Graldi, Jacopo and Giordano, Marco and Hofmann, Thomas and Anagnostidis, Sotiris and Singh, Sidak Pal},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@inproceedings{zhang2025beyond,\n",
    "  title={Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion},\n",
    "  author={Zhang, Binchi and Zheng, Zaiyi and Chen, Zhengzhang and Li, Jundong},\n",
    "  booktitle={Forty-second International Conference on Machine Learning},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{zhou2023neural,\n",
    "  title={Neural functional transformers},\n",
    "  author={Zhou, Allan and Yang, Kaien and Jiang, Yiding and Burns, Kaylee and Xu, Winnie and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},\n",
    "  journal={Advances in neural information processing systems},\n",
    "  volume={36},\n",
    "  pages={77485--77502},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{lim2024graph,\n",
    "  title={Graph Metanetworks for Processing Diverse Neural Architectures},\n",
    "  author={Lim, Derek and Maron, Haggai and Law, Marc T and Lorraine, Jonathan and Lucas, James},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{zhou2023permutation,\n",
    "  title={Permutation equivariant neural functionals},\n",
    "  author={Zhou, Allan and Yang, Kaien and Burns, Kaylee and Cardace, Adriano and Jiang, Yiding and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},\n",
    "  journal={Advances in neural information processing systems},\n",
    "  volume={36},\n",
    "  pages={24966--24992},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{tran2025equivariant,\n",
    "  title={Equivariant Neural Functional Networks for Transformers},\n",
    "  author={Tran, Hoang V and Vo, Thieu and Huu, Tho Tran and Nguyen-Nhat, Minh-Khoi and Tran, Thanh and Pham, Duy-Tung and Nguyen, Tan Minh and others},\n",
    "  booktitle={The Thirteenth International Conference on Learning Representations},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{ziyin2024parameter,\n",
    "  title={Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent},\n",
    "  author={Ziyin, Liu and Wang, Mingze and Li, Hongchao and Wu, Lei},\n",
    "  journal={arXiv preprint arXiv:2402.07193},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@inproceedings{ziyin2024symmetry,\n",
    "  title={Symmetry Induces Structure and Constraint of Learning},\n",
    "  author={Liu, Ziyin},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@article{naveed2025comprehensive,\n",
    "  title={A comprehensive overview of large language models},\n",
    "  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},\n",
    "  journal={ACM Transactions on Intelligent Systems and Technology},\n",
    "  volume={16},\n",
    "  number={5},\n",
    "  pages={1--72},\n",
    "  year={2025}\n",
    "}\n",
    "\n",
    "@article{kasneci2023chatgpt,\n",
    "  title={ChatGPT for good? On opportunities and challenges of large language models for education},\n",
    "  author={Kasneci, Enkelejda and Se{\\ss}ler, Kathrin and K{\\\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\\\"u}nnemann, Stephan and H{\\\"u}llermeier, Eyke and others},\n",
    "  journal={Learning and individual differences},\n",
    "  volume={103},\n",
    "  pages={102274},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@article{thirunavukarasu2023large,\n",
    "  title={Large language models in medicine},\n",
    "  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},\n",
    "  journal={Nature medicine},\n",
    "  volume={29},\n",
    "  number={8},\n",
    "  pages={1930--1940},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@article{han2024parameter,\n",
    "  title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey},\n",
    "  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},\n",
    "  journal={Transactions on Machine Learning Research},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@inproceedings{hu2022lora,\n",
    "  title={LoRA: Low-Rank Adaptation of Large Language Models},\n",
    "  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},\n",
    "  booktitle={International Conference on Learning Representations},\n",
    "  year={2022}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "bibtex_b = \"\"\"\n",
    "% Use this file for citations not found in the ACL Anthology (contained in \"anthology.bib\").\n",
    "\n",
    "@book{Aho:72,\n",
    "    author  = {Alfred V. Aho and Jeffrey D. Ullman},\n",
    "    title   = {The Theory of Parsing, Translation and Compiling},\n",
    "    year    = \"1972\",\n",
    "    volume  = \"1\",\n",
    "    publisher = {Prentice-Hall},\n",
    "    address = {Englewood Cliffs, NJ}\n",
    "}\n",
    "\n",
    "@book{APA:83,\n",
    "    author  = {{American Psychological Association}},\n",
    "    title   = {Publications Manual},\n",
    "    year    = \"1983\",\n",
    "    publisher = {American Psychological Association},\n",
    "    address = {Washington, DC}\n",
    "}\n",
    "@inproceedings{hu2022lora,\n",
    "  title={LoRA: Low-Rank Adaptation of Large Language Models},\n",
    "  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},\n",
    "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2022},\n",
    "  url={https://openreview.net/forum?id=nZeVKeeFYf9}\n",
    "}\n",
    "\n",
    "@inproceedings{wortsman2022model,\n",
    "  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},\n",
    "  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},\n",
    "  booktitle={International Conference on Machine Learning},\n",
    "  pages={23965--23998},\n",
    "  year={2022},\n",
    "  organization={PMLR}\n",
    "}\n",
    "\n",
    "@article{ilharco2023editing,\n",
    "  title={Editing models with task arithmetic},\n",
    "  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},\n",
    "  journal={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@article{yadav2024ties,\n",
    "  title={TIES-Merging: Resolving Interference When Merging Models},\n",
    "  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin and Bansal, Mohit},\n",
    "  journal={Advances in Neural Information Processing Systems},\n",
    "  volume={36},\n",
    "  year={2024}\n",
    "}\n",
    "\n",
    "@inproceedings{zhang2023adalora,\n",
    "  title={Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},\n",
    "  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},\n",
    "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{wang2018glue,\n",
    "  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
    "  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
    "  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},\n",
    "  pages={353--355},\n",
    "  year={2018}\n",
    "}\n",
    "\n",
    "@article{zhang2025rotation,\n",
    "  title={Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion},\n",
    "  author={Zhang, Binchi and Zheng, Zaiyi and Chen, Zhengzhang and Li, Jundong},\n",
    "  journal={Proceedings of the 42nd International Conference on Machine Learning},\n",
    "  year={2025},\n",
    "  organization={PMLR}\n",
    "}\n",
    "\n",
    "@article{entezari2022git,\n",
    "  title={The role of permutation invariance in linear mode connectivity of neural networks},\n",
    "  author={Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},\n",
    "  journal={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@inproceedings{ainsworth2023git,\n",
    "  title={Git re-basin: Merging models modulo permutation symmetries},\n",
    "  author={Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddharth},\n",
    "  booktitle={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@inproceedings{singh2020federated,\n",
    "  title={Model fusion via optimal transport},\n",
    "  author={Singh, Sidak Pal and Jaggi, Martin},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  volume={33},\n",
    "  pages={22045--22055},\n",
    "  year={2020}\n",
    "}\n",
    "\n",
    "@inproceedings{matena2022merging,\n",
    "  title={Merging models with fisher-weighted averaging},\n",
    "  author={Matena, Michael and Raffel, Colin},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  volume={35},\n",
    "  pages={17703--17716},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "@article{jin2023dataless,\n",
    "  title={Dataless Knowledge Fusion by Merging Weights of Language Models},\n",
    "  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},\n",
    "  journal={Proceedings of the International Conference on Learning Representations},\n",
    "  year={2023}\n",
    "}\n",
    "\n",
    "@article{Chandra:81,\n",
    "\tauthor = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},\n",
    "\tyear = \"1981\",\n",
    "\ttitle = {Alternation},\n",
    "\tjournal = {Journal of the Association for Computing Machinery},\n",
    "\tvolume = \"28\",\n",
    "\tnumber = \"1\",\n",
    "\tpages = \"114--133\",\n",
    "\tdoi = \"10.1145/322234.322243\",\n",
    "}\n",
    "\n",
    "@inproceedings{andrew2007scalable,\n",
    "  title={Scalable training of {L1}-regularized log-linear models},\n",
    "  author={Andrew, Galen and Gao, Jianfeng},\n",
    "  booktitle={Proceedings of the 24th International Conference on Machine Learning},\n",
    "  pages={33--40},\n",
    "  year={2007},\n",
    "}\n",
    "\n",
    "@book{Gusfield:97,\n",
    "    author  = {Dan Gusfield},\n",
    "    title   = {Algorithms on Strings, Trees and Sequences},\n",
    "    year    = \"1997\",\n",
    "    publisher = {Cambridge University Press},\n",
    "    address = {Cambridge, UK}\n",
    "}\n",
    "\n",
    "@article{rasooli-tetrault-2015,\n",
    "    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},\n",
    "    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},\n",
    "    journal   = {Computing Research Repository},\n",
    "    volume    = {arXiv:1503.06733},\n",
    "    year      = {2015},\n",
    "    url       = {http://arxiv.org/abs/1503.06733},\n",
    "    note    = {version 2}\n",
    "}\n",
    "\n",
    "@article{Ando2005,\n",
    "\tAcmid = {1194905},\n",
    "\tAuthor = {Ando, Rie Kubota and Zhang, Tong},\n",
    "\tIssn = {1532-4435},\n",
    "\tIssue_Date = {12/1/2005},\n",
    "\tJournal = {Journal of Machine Learning Research},\n",
    "\tMonth = dec,\n",
    "\tNumpages = {37},\n",
    "\tPages = {1817--1853},\n",
    "\tPublisher = {JMLR.org},\n",
    "\tTitle = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},\n",
    "\tVolume = {6},\n",
    "\tYear = {2005}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "merged = merge_bibtex(bibtex_a, bibtex_b)\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad770d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
